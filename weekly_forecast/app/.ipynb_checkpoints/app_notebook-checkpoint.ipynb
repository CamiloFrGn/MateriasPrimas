{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c089c078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries success\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "# Importación de las librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import time\n",
    "import urllib\n",
    "import sys\n",
    "import winsound\n",
    "import sqlalchemy as sa\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "import random\n",
    "from io import BytesIO\n",
    "import modulo_conn_sql as mcq\n",
    "from prophet import Prophet\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from livelossplot import PlotLossesKeras\n",
    "import keras.optimizers as opts\n",
    "from keras import backend as K\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from scipy import stats\n",
    "from scipy.stats import johnsonsu   \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"libraries success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff5a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running neural network\n",
      "(2994, 15, 6)\n",
      "15\n",
      "6\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 28s 43ms/step - loss: 0.0197\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0137\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 15s 39ms/step - loss: 0.0130\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.0116\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 12s 33ms/step - loss: 0.0107\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "facebook prophet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:52:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final forecast is: 42230.0\n",
      "running daily forecast\n",
      "---------------------------------\n",
      "sending daily forecast to sql\n",
      "running parameters update\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def play_error_alert():\n",
    "    winsound.PlaySound(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\sounds\\mixkit-wrong-answer-bass-buzzer-948.wav',winsound.SND_FILENAME)\n",
    "\n",
    "\n",
    "def play_finished_alert():\n",
    "    frequency = 1000  # Frequency of the finished alert sound\n",
    "    duration = 500  # Duration of the sound in milliseconds\n",
    "    winsound.Beep(frequency, duration)\n",
    "\n",
    "\n",
    "################ RUN NEURAL NETWORK #################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    print(\"running neural network\")\n",
    "\n",
    "    # Metodos auxiliares\n",
    "    def conectarSQL():\n",
    "        conn = mcq.ConexionSQL()\n",
    "        cursor = conn.getCursor()\n",
    "        return cursor\n",
    "\n",
    "    #Query BD SQL-Server Cemex\n",
    "    def querySQL(query, parametros):\n",
    "        #Conectar con base sql y ejecutar consulta\n",
    "        cursor = conectarSQL()\n",
    "        try:\n",
    "            cursor.execute(query, parametros)\n",
    "            #obtener nombre de columnas\n",
    "            names = [ x[0] for x in cursor.description]\n",
    "\n",
    "            #Reunir todos los resultado en rows\n",
    "            rows = cursor.fetchall()\n",
    "            resultadoSQL = []\n",
    "\n",
    "            #Hacer un array con los resultados\n",
    "            while rows:\n",
    "                resultadoSQL.append(rows)\n",
    "                if cursor.nextset():\n",
    "                    rows = cursor.fetchall()\n",
    "                else:\n",
    "                    rows = None\n",
    "\n",
    "            #Redimensionar el array para que quede en dos dimensiones\n",
    "            resultadoSQL = np.array(resultadoSQL)\n",
    "            resultadoSQL = np.reshape(resultadoSQL, (resultadoSQL.shape[1], resultadoSQL.shape[2]) )\n",
    "        finally:\n",
    "                if cursor is not None:\n",
    "                    cursor.close()\n",
    "\n",
    "\n",
    "        return pd.DataFrame(resultadoSQL, columns = names)\n",
    "\n",
    "    def get_dataset(\n",
    "        procedure_name: str,\n",
    "        param_pais: str,\n",
    "        param_fecha_inicio: datetime,\n",
    "        param_fecha_fin: datetime ) -> pd.core.frame.DataFrame:\n",
    "\n",
    "        df = querySQL( \n",
    "            \"{CALL \" + procedure_name+ \" (?,?,?)}\", \n",
    "            ( \n",
    "                param_pais, \n",
    "                param_fecha_inicio.strftime(\"%Y-%m-%d\"), \n",
    "                param_fecha_fin.strftime(\"%Y-%m-%d\") \n",
    "            ) \n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    def split_train_test(\n",
    "        param_df: pd.core.frame.DataFrame, \n",
    "        date_split:str ) -> (list, list):\n",
    "\n",
    "        # Convertir dataframe a lista de listas, solo se toman las columnas desde el volumen\n",
    "        dataset = param_df.iloc[: , 1:].values\n",
    "\n",
    "        # definicion del conjunto de entrenamiento\n",
    "        dataset_train = df[ df['Fecha'] < date_split ]\n",
    "        dataset_train = dataset_train.iloc[:,2:].values\n",
    "\n",
    "        # definicion del conjunto de test\n",
    "        dataset_test = df[ df['Fecha'] >= date_split ]\n",
    "        dataset_test = dataset_test.iloc[:,2:].values\n",
    "\n",
    "        return (dataset_train, dataset_test)\n",
    "\n",
    "    def scale_train_test(param_df, param_train, param_test ):\n",
    "\n",
    "        dataset = param_df.iloc[: , 2:].values\n",
    "        # Definicion de variable para escalar los datos entre 0 y 1\n",
    "        sc = MinMaxScaler(feature_range = (0, 1))\n",
    "        dataset = sc.fit(dataset)\n",
    "\n",
    "        # Ajuste de los datos segun la variable escaladora\n",
    "        dataset_train = sc.transform(param_train)\n",
    "        dataset_test = sc.transform(param_test)\n",
    "\n",
    "        # save the scaler\n",
    "        dump(sc, open(r\"C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\datos/\" + pais + \"/\" + pais +'.pkl', 'wb'))\n",
    "\n",
    "        return (dataset_train, dataset_test) \n",
    "\n",
    "    def train_model(\n",
    "        param_dataset: list,\n",
    "        timesteps : int,\n",
    "        layers : int,\n",
    "        units: int,\n",
    "        dropout: float, \n",
    "        epochs: int, \n",
    "        batch: int,\n",
    "        pais: str\n",
    "    )-> None:\n",
    "\n",
    "        predictor_variables_num = param_dataset.shape[1] #obtener numero de variables \n",
    "\n",
    "        #define array by predictor variable\n",
    "        variables = [ [] for i in range(0, predictor_variables_num) ]\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "\n",
    "        for i in range(timesteps, param_dataset.shape[0] ):\n",
    "\n",
    "            # process for each variable\n",
    "            for j in range(0, predictor_variables_num):\n",
    "\n",
    "                variables[j].append( param_dataset[i-timesteps:i, j] )\n",
    "\n",
    "            y_train.append(param_dataset[i, 0])\n",
    "\n",
    "        #convert to numpy objects\n",
    "        for i in range(0, predictor_variables_num):\n",
    "            variables[i] = np.array(variables[i])\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        #reshape numpy objects\n",
    "        for i in range(0, predictor_variables_num):\n",
    "            variables[i] = np.reshape(variables[i], (variables[i].shape[0], variables[i].shape[1], 1 ))\n",
    "\n",
    "        #build tensor structure for LSTM\n",
    "\n",
    "        # if just one variable\n",
    "        if predictor_variables_num == 1:\n",
    "            X_train = variables[0]\n",
    "\n",
    "        else:\n",
    "            X_train = np.append(variables[0], (variables[1]), axis=2)\n",
    "\n",
    "        # append to x_train if more than 2 variables\n",
    "        if predictor_variables_num > 2:\n",
    "            for i in range(2, predictor_variables_num):\n",
    "\n",
    "                    X_train = np.append(X_train, (variables[i]), axis=2)\n",
    "\n",
    "        #inizializate regressor\n",
    "        regressor = Sequential()\n",
    "\n",
    "        #if there is more than a layer, return input dimension to the next layer, through return_sequence parameter\n",
    "        rs = True if layers > 1  else False\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(X_train.shape[1])\n",
    "        print(X_train.shape[2])\n",
    "        for i in range( 0, layers):\n",
    "\n",
    "            # if the first layer, define input dimensions\n",
    "            if i == 0:\n",
    "                regressor.add(LSTM(units=units, return_sequences = rs, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "            else:\n",
    "                regressor.add(LSTM(units=units, return_sequences = rs))\n",
    "\n",
    "            #if dropout layers\n",
    "            if dropout > 0.0:\n",
    "                regressor.add(Dropout(dropout))\n",
    "\n",
    "            # penultimate layer dont return input dimensions, because the last one just has 1 neuron\n",
    "            if i == layers -2: \n",
    "                rs = False\n",
    "\n",
    "        #output layer\n",
    "        regressor.add(Dense(units=1))\n",
    "\n",
    "        #compile RNR\n",
    "        regressor.compile(optimizer = 'adam', loss='mean_squared_error')\n",
    "\n",
    "        ##regressor.summary()\n",
    "\n",
    "        #fit the RNR to datatrain\n",
    "        regressor.fit(X_train, y_train, \n",
    "                      epochs=epochs, \n",
    "                      batch_size = batch )\n",
    "\n",
    "        #save model\n",
    "        regressor.save(r\"C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\datos/\" + pais +\"/\" + pais + \"_testing.h5\")\n",
    "\n",
    "    def test_model(\n",
    "        param_train : list, \n",
    "        param_test: list, \n",
    "        test_no_scale : list,\n",
    "        timesteps: int,\n",
    "        pais: str ):\n",
    "\n",
    "        predictor_variables_num = param_train.shape[1] #obtener cantidad de elementos dentro de cada dimension\n",
    "\n",
    "        # load regressor in testing\n",
    "        regressor_test = load_model(r\"C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\datos/\" + pais +\"/\" + pais + \"_testing.h5\")\n",
    "\n",
    "        # load the scaler\n",
    "        scaler = load(open(r\"C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\datos/\" + pais + \"/\" + pais + '.pkl', 'rb'))\n",
    "\n",
    "        #inputs are last timesteps for first prediction day\n",
    "        inputs = param_train[len(param_train) - timesteps: ]\n",
    "\n",
    "        #this process will be excecute for every prediction day\n",
    "        for j in range (0, param_test.shape[0] ):\n",
    "\n",
    "            X_test = []\n",
    "            #define array by predictor variable\n",
    "            variables = [ [] for i in range(0, predictor_variables_num) ]\n",
    "\n",
    "            for i in range(timesteps, inputs.shape[0]+1 ):\n",
    "\n",
    "                # process for each variable\n",
    "                for k in range(0, predictor_variables_num):\n",
    "\n",
    "                    variables[k].append( inputs[i-timesteps:i, k] )\n",
    "\n",
    "            #convert to numpy objects\n",
    "            for i in range(0, predictor_variables_num):\n",
    "                variables[i] = np.array(variables[i])\n",
    "\n",
    "            #reshape numpy objects\n",
    "            for i in range(0, predictor_variables_num):\n",
    "                variables[i] = np.reshape(variables[i], (variables[i].shape[0], variables[i].shape[1], 1 ))\n",
    "\n",
    "\n",
    "            #build tensor structure for LSTM\n",
    "            # if just one variable\n",
    "            if predictor_variables_num == 1:\n",
    "                X_test = variables[0]\n",
    "\n",
    "            else:\n",
    "                X_test = np.append(variables[0], (variables[1]), axis=2)\n",
    "\n",
    "            # append to x_test if more than 2 variables\n",
    "            if predictor_variables_num > 2:\n",
    "                for i in range(2, predictor_variables_num):\n",
    "\n",
    "                    X_test = np.append(X_test, (variables[i]), axis=2)\n",
    "\n",
    "            #make prediction\n",
    "            prediction = regressor_test.predict(X_test)\n",
    "\n",
    "            #to prediction append another regressor variables\n",
    "            prediction = np.append(prediction, (param_test[:len(prediction), 1 : ]), axis=1 )\n",
    "\n",
    "            inputs = param_train[len(param_train) - timesteps:]\n",
    "\n",
    "            inputs = np.append(inputs, (prediction), axis=0 )\n",
    "\n",
    "\n",
    "        print(\"MSE Test: \"+str(mean_squared_error(prediction,param_test)))\n",
    "\n",
    "        prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    def forecast(\n",
    "        df_forecast: pd.core.frame.DataFrame,\n",
    "        start_date_forecast : datetime ):\n",
    "\n",
    "        dataset_pred = df_forecast[['DiaSemana','DiaSemana', 'Mes', 'Semana_Relativa', 'Semanas_mes', 'Año']]\n",
    "\n",
    "        #convert to list\n",
    "        predict_set = dataset_pred.iloc[:,:].values\n",
    "\n",
    "        # load the scaler\n",
    "        scaler = load(open(r\"C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\datos/\" + pais + \"/\" + pais + '.pkl', 'rb'))\n",
    "\n",
    "        # load regressor in testing\n",
    "        regressor_test = load_model(r\"C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\datos/\" + pais +\"/\" + pais + \"_testing.h5\")\n",
    "\n",
    "        #scale forecast dataset \n",
    "        predict_set_scaled = scaler.transform(predict_set)\n",
    "\n",
    "        #define input dataset to start forecast\n",
    "        dataset_test = df[ df['Fecha'] < start_date_forecast ]\n",
    "        test_set  = dataset_test.iloc[:, 2:].values\n",
    "        # scale features\n",
    "        test_set_scaled = scaler.transform(test_set)\n",
    "\n",
    "        #los inputs van a ser los ultimos Timesteps dias del training set, para predecir el primer dia \n",
    "        inputs = test_set_scaled[len(test_set_scaled) - timesteps: ]\n",
    "\n",
    "        predictor_variables_num = predict_set_scaled.shape[1] \n",
    "\n",
    "        #this process will be excecute for every prediction day\n",
    "        for j in range (0, predict_set_scaled.shape[0] ):\n",
    "            X_test = []\n",
    "            #define array by predictor variable\n",
    "            variables = [ [] for i in range(0, predictor_variables_num) ]\n",
    "\n",
    "            for i in range(timesteps, inputs.shape[0]+1 ):\n",
    "\n",
    "                    # process for each variable\n",
    "                    for k in range(0, predictor_variables_num):\n",
    "\n",
    "                        variables[k].append( inputs[i-timesteps:i, k] )\n",
    "            #convert to numpy objects\n",
    "            for i in range(0, predictor_variables_num):\n",
    "                variables[i] = np.array(variables[i])\n",
    "\n",
    "            #reshape numpy objects\n",
    "            for i in range(0, predictor_variables_num):\n",
    "                variables[i] = np.reshape(variables[i], (variables[i].shape[0], variables[i].shape[1], 1 ))\n",
    "\n",
    "            #build tensor structure for LSTM\n",
    "            # if just one variable\n",
    "            if predictor_variables_num == 1:\n",
    "                X_test = variables[0]\n",
    "\n",
    "            else:\n",
    "                X_test = np.append(variables[0], (variables[1]), axis=2)\n",
    "\n",
    "            #append to x_test if more than 2 variables\n",
    "            if predictor_variables_num > 2:\n",
    "                for i in range(2, predictor_variables_num):\n",
    "\n",
    "                    X_test = np.append(X_test, (variables[i]), axis=2)     \n",
    "\n",
    "            #make prediction\n",
    "            prediction = regressor_test.predict(X_test)\n",
    "\n",
    "            #to prediction append another regressor variables\n",
    "            prediction = np.append(prediction, (predict_set_scaled[:len(prediction), 1 : ]), axis=1 )\n",
    "\n",
    "            inputs = test_set_scaled[len(test_set_scaled) - timesteps:]\n",
    "\n",
    "            inputs = np.append(inputs, (prediction), axis=0 )\n",
    "\n",
    "        prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    #Run parameters for neural network\n",
    "    #Parametros SQL\n",
    "    pais = 'Colombia'  \n",
    "    inicioHistoria = datetime.datetime(2013,1, 1) #'2013-05-01'\n",
    "    finHistoria = datetime.datetime.today() #fecha actual\n",
    "    fecha_split = finHistoria - datetime.timedelta(days=30)\n",
    "\n",
    "    df = get_dataset(\"SCAC_AP4_Serie_VolumenDiario\", pais, inicioHistoria, finHistoria)\n",
    "\n",
    "    #SPLIT TRAIN - TEST\n",
    "    dataset_train_nscale, dataset_test_nscale = split_train_test(df, fecha_split) \n",
    "\n",
    "    #SCALE VARIABLES\n",
    "    dataset_train, dataset_test = scale_train_test(df, dataset_train_nscale,dataset_test_nscale ) \n",
    "\n",
    "    #TRAIN MODEL\n",
    "\n",
    "    timesteps = 15\n",
    "    layers = 5\n",
    "    units = 30\n",
    "    dropout = 0.2\n",
    "    epochs = 5\n",
    "    batch = 8\n",
    "\n",
    "    train_model(dataset_train, timesteps, layers, units, dropout, epochs, batch, pais)\n",
    "\n",
    "    #Parameters for all forecast \n",
    "\n",
    "    start_date_forecast = datetime.datetime.today() + datetime.timedelta(days=1)\n",
    "    end_date_forecast = start_date_forecast + datetime.timedelta(days=8)\n",
    "\n",
    "    start_date_forecast_str = start_date_forecast.strftime(\"%Y-%m-%d\")\n",
    "    end_date_forecast_str = end_date_forecast.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    start_date_forecast_year = start_date_forecast.year\n",
    "\n",
    "\n",
    "\n",
    "    df2 = get_dataset(\"SCAC_AP4_Serie_VolumenDiario_AuxFecha\", pais, start_date_forecast, end_date_forecast)\n",
    "\n",
    "    result = forecast(df2, start_date_forecast)\n",
    "\n",
    "    #mensual results\n",
    "    df_result = pd.DataFrame({'Forecast':result[:, 0]})\n",
    "    df_result = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "    neural_network_forecast_concret_value = df_result['Forecast'].sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############## RUN FACEBOOK PROPHET MODEL ####################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"facebook prophet\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Metodos auxiliares\n",
    "    def conectarSQL():\n",
    "        conn = mcq.ConexionSQL()\n",
    "        cursor = conn.getCursor()\n",
    "        return cursor\n",
    "\n",
    "    #Query BD SQL-Server Cemex\n",
    "    def querySQL(query, parametros):\n",
    "        #Conectar con base sql y ejecutar consulta\n",
    "        cursor = conectarSQL()\n",
    "        try:\n",
    "            cursor.execute(query, parametros)\n",
    "            #obtener nombre de columnas\n",
    "            names = [ x[0] for x in cursor.description]\n",
    "\n",
    "            #Reunir todos los resultado en rows\n",
    "            rows = cursor.fetchall()\n",
    "            resultadoSQL = []\n",
    "\n",
    "            #Hacer un array con los resultados\n",
    "            while rows:\n",
    "                resultadoSQL.append(rows)\n",
    "                if cursor.nextset():\n",
    "                    rows = cursor.fetchall()\n",
    "                else:\n",
    "                    rows = None\n",
    "\n",
    "            #Redimensionar el array para que quede en dos dimensiones\n",
    "            resultadoSQL = np.array(resultadoSQL)\n",
    "            resultadoSQL = np.reshape(resultadoSQL, (resultadoSQL.shape[1], resultadoSQL.shape[2]) )\n",
    "        finally:\n",
    "                if cursor is not None:\n",
    "                    cursor.close()\n",
    "        return pd.DataFrame(resultadoSQL, columns = names)\n",
    "\n",
    "    def get_dataset(\n",
    "        procedure_name: str,\n",
    "        param_pais: str,\n",
    "        param_fecha_inicio: datetime,\n",
    "        param_fecha_fin: datetime ) -> pd.core.frame.DataFrame:\n",
    "\n",
    "        df = querySQL( \n",
    "            \"{CALL \" + procedure_name+ \" (?,?,?)}\", \n",
    "            ( \n",
    "                param_pais, \n",
    "                param_fecha_inicio.strftime(\"%Y-%m-%d\"), \n",
    "                param_fecha_fin.strftime(\"%Y-%m-%d\") \n",
    "            ) \n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    dias_pronostico = 10\n",
    "    end_date_calendar = finHistoria + datetime.timedelta(days=dias_pronostico)\n",
    "\n",
    "\n",
    "    df = df.sort_values(by=\"Fecha\")\n",
    "    df = df[['Fecha', 'Vol']]\n",
    "    df.columns = ['ds', 'y']\n",
    "\n",
    "    #run model\n",
    "    #Creacion y ajuste del modelo\n",
    "    m = Prophet(\n",
    "        interval_width = 0.85,\n",
    "        growth = 'linear',\n",
    "        seasonality_mode = 'multiplicative',\n",
    "        changepoint_prior_scale=30,\n",
    "        seasonality_prior_scale=35,\n",
    "        holidays_prior_scale=35,\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        yearly_seasonality=False,\n",
    "        ).add_seasonality(\n",
    "            name='monthly',\n",
    "            period=30.5,\n",
    "            fourier_order=10\n",
    "        ).add_seasonality(\n",
    "            name='weekly',\n",
    "            period=7,\n",
    "            fourier_order=75\n",
    "        ).add_seasonality(\n",
    "            name='yearly',\n",
    "            period=365.25,\n",
    "            fourier_order=90\n",
    "        )\n",
    "\n",
    "    m.fit(df)\n",
    "\n",
    "    #pronostico\n",
    "    future = m.make_future_dataframe(periods=dias_pronostico, freq = 'D')\n",
    "\n",
    "    forecast = m.predict(future)\n",
    "    df_real_prediccion = forecast[['ds','yhat']]\n",
    "\n",
    "    df_real_prediccion = df_real_prediccion[df_real_prediccion['ds'] >= start_date_forecast_str]\n",
    "    df_real_prediccion = df_real_prediccion[df_real_prediccion['ds'] <= end_date_forecast_str]\n",
    "\n",
    "    prophet_forecast_concret_value = df_real_prediccion['yhat'].sum()\n",
    "\n",
    "    #conseno entre ambos modelos\n",
    "    \n",
    "    print(\"RNR forecast: \"+str(neural_network_forecast_concret_value))\n",
    "    print(\"Prophet forecast: \"+str(prophet_forecast_concret_value))\n",
    "    forecast_concret_final = round((neural_network_forecast_concret_value+prophet_forecast_concret_value)/2,0)\n",
    "\n",
    "    print(\"Final forecast is: \"+str(forecast_concret_final))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### RUN DAILY FORECAST #####################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"running daily forecast\")\n",
    "\n",
    "\n",
    "\n",
    "    #---------------------SQLALCHEMY CONNECTION---------------------------------\n",
    "    params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=USCLDBITVMP01;DATABASE=BI_Tableau;UID=usertableau;PWD=usertableau$\")\n",
    "    engine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params, fast_executemany=True)\n",
    "    #---------------------------------------------------------------------------\n",
    "\n",
    "    def send_df_to_sql(data,database_name):\n",
    "        try:          \n",
    "            data.to_sql(database_name, engine, index=False, if_exists=\"append\", schema=\"dbo\")  \n",
    "\n",
    "            return \"success\"       \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            sys.exit()\n",
    "\n",
    "\n",
    "    def conectarSQL():\n",
    "        conn = mcq.ConexionSQL()\n",
    "        cursor = conn.getCursor()\n",
    "        return cursor\n",
    "\n",
    "    #Query BD SQL-Server Cemex\n",
    "    def querySQL(query, parametros):\n",
    "        #Conectar con base sql y ejecutar consulta\n",
    "        cursor = conectarSQL()\n",
    "        try:\n",
    "            cursor.execute(query, parametros)\n",
    "            #obtener nombre de columnas\n",
    "            names = [ x[0] for x in cursor.description]\n",
    "\n",
    "            #Reunir todos los resultado en rows\n",
    "            rows = cursor.fetchall()\n",
    "            resultadoSQL = []\n",
    "\n",
    "            #Hacer un array con los resultados\n",
    "            while rows:\n",
    "                resultadoSQL.append(rows)\n",
    "                if cursor.nextset():\n",
    "                    rows = cursor.fetchall()\n",
    "                else:\n",
    "                    rows = None\n",
    "\n",
    "            #Redimensionar el array para que quede en dos dimensiones\n",
    "            resultadoSQL = np.array(resultadoSQL)\n",
    "            resultadoSQL = np.reshape(resultadoSQL, (resultadoSQL.shape[1], resultadoSQL.shape[2]) )\n",
    "        finally:\n",
    "                if cursor is not None:\n",
    "                    cursor.close()\n",
    "        return pd.DataFrame(resultadoSQL, columns = names)\n",
    "\n",
    "    # dataset(pandas dataframe): base de datos con el historico\n",
    "    # array_group_top(array) : array de la jerarquia mas ALTA con el nombre de las columnas del dataset por el cual se quiere agrupar las proporciones\n",
    "    # array_group_bottom(array) : array de la jerarquia mas BAJA con el nombre de las columnas del dataset por el cual se quiere agrupar las proporciones\n",
    "    # medida_target( string ) : nombre de la columna que contiene los datos objetivo de la proporcion\n",
    "    # group_target(array) : array de nombre de columnas con las cuales queremos la proporcion final\n",
    "    # name_proportion(string) : etiqueta de la dimension a la cual le estamos calculando la proporcion\n",
    "\n",
    "    def historical_proportion( dataset, array_group_top, array_group_bottom, medida_target, group_target, name_proportion  ):\n",
    "\n",
    "        promedio_group_top = dataset.groupby(array_group_top)[medida_target].mean().reset_index()\n",
    "\n",
    "        promedio_group_bottom = dataset.groupby(array_group_bottom)[medida_target].mean().reset_index()    \n",
    "\n",
    "        proportion  = pd.merge(promedio_group_bottom, promedio_group_top, how = 'inner', left_on = array_group_top, right_on = array_group_top )\n",
    "\n",
    "        proportion['proportion'] = proportion[medida_target + '_x'] / proportion[medida_target + '_y']\n",
    "\n",
    "        proportion = proportion.groupby(group_target)['proportion'].median().reset_index()\n",
    "\n",
    "        proportion.rename(columns={'proportion':'proportion_' + name_proportion}, inplace = True)\n",
    "\n",
    "        return proportion \n",
    "\n",
    "    def random_number (num):\n",
    "\n",
    "        num = num if num <= 1 else 1\n",
    "\n",
    "        return 1 + random.uniform( 0, num) if  random.random() < 0.5 else 1 - random.uniform( 0, num) \n",
    "\n",
    "    #desviacion estandar, media y coeficiente de variacion\n",
    "    def stats_serie(dataset, array_group, colum_target):\n",
    "        ret = dataset.groupby(array_group)[colum_target].agg( ['std', 'mean']).reset_index()\n",
    "        ret['cov'] = ret['std']/ret['mean']\n",
    "\n",
    "        return ret\n",
    "\n",
    "    #PRINCIPAL PARAMETROS\n",
    "\n",
    "    volPais = forecast_concret_final\n",
    "    inicioHistoria = finHistoria - datetime.timedelta(days=180) #'2013-05-01'\n",
    "\n",
    "\n",
    "    #Targets\n",
    "    nivel_volatilidad = 0.0\n",
    "    criterio_historia_reciente = 90 #dias\n",
    "\n",
    "    version = \"AUTO_SEMANAL\"\n",
    "\n",
    "    absorcionEstadistica = 1 #criterio para tomar las proporciones historicas de las plantas o False -> archivo que define el volumen por planta\n",
    "\n",
    "    # FIN PARAMETROS \n",
    "\n",
    "    #Consulta de datos en la base SQL\n",
    "    despachosSQL = querySQL(  \"{CALL SCAC_AP8_BaseForecast (?,?,?)}\", (pais, inicioHistoria.strftime(\"%Y-%m-%d\"), finHistoria.strftime(\"%Y-%m-%d\") ) )\n",
    "    #despachosSQL = despachosSQL[despachosSQL['Planta']!= 'G014']\n",
    "    calendarioLogistico = querySQL( \"{CALL SCAC_AP9_CalendarioLogistico_auto (?,?,?)}\" , (pais, start_date_forecast_str, end_date_forecast_str))\n",
    "    #agrego informacion geografica        \n",
    "    nombre_cluster = querySQL( \"SELECT Centro, Ciudad_Cluster as Ciudad, [Desc Cluster] as Cluster, [Planta Unica] as PlantaUnica FROM SCAC_AT1_NombreCluster where Pais = ? and Activo = 1\" , (pais) )\n",
    "\n",
    "    #Otras Consultas\n",
    "    irregularidades  = pd.read_excel(r\"C:\\Users\\snortiz\\OneDrive - CEMEX\\Documentos\\Proyectos\\Proyectos-Cemex\\Proyectos-Cemex\\Desagregacion_Forecast\\app\\datos\\BaseIrregularidades.xlsx\")\n",
    "    irregularidades['FechaEntrega'] = pd.to_datetime(irregularidades['FechaEntrega'])\n",
    "\n",
    "    #arreglo de formatos\n",
    "    despachosSQL['totalEntregado'] = despachosSQL['totalEntregado'].astype(float)\n",
    "    calendarioLogistico['Dia_Semana'] = calendarioLogistico['Dia_Semana'].astype(int)\n",
    "\n",
    "    #SOLO CENTROS ACTIVOS\n",
    "    despachosSQL = pd.merge(despachosSQL,nombre_cluster[['Centro']], left_on='Planta', right_on='Centro' ).drop(columns=['Centro'])\n",
    "\n",
    "    #irregularidades a nivel pais\n",
    "    irr_nacional = irregularidades[irregularidades['Ciudad']==pais]\n",
    "    despachos_irregularidades = pd.merge(despachosSQL, nombre_cluster, left_on='Planta', right_on='Centro')\n",
    "    despachos_irregularidades = pd.merge(despachos_irregularidades, irr_nacional[['FechaEntrega','EtiquetaIrregularidad']], on='FechaEntrega', how='left')\n",
    "\n",
    "    def generardor_desagregacion(despachosSQL, calendarioLogistico, absorcionEstadistica, criterio_historia_reciente, inicioHistoria, finHistoria, nivel_volatilidad, volPais, pais , despachos_irregularidades, irr_nacional):\n",
    "\n",
    "        #Proporciones con toda la historia\n",
    "\n",
    "        \"\"\"NOTAS: se obetiene las proporciones a nivel general de los ultimos 30 dias de la fecha final de historia establecido en los parametros.\"\"\"\n",
    "        proportion_plant = historical_proportion(despachosSQL[despachosSQL['FechaEntrega'] >= (finHistoria - datetime.timedelta(criterio_historia_reciente))], ['Año', 'Mes'], ['Año', 'Mes', 'Planta'], 'totalEntregado', ['Planta'], 'planta')\n",
    "\n",
    "        \"\"\"NOTAS: se obetiene las proporciones a nivel de semana de toda la historia establecida en los parametros\"\"\"\n",
    "        proportion_week = historical_proportion(despachosSQL, ['Año', 'Mes', 'Planta'], ['Año', 'Mes', 'Planta', 'Semana_Relativa'], 'totalEntregado', ['Planta', 'Semana_Relativa'], 'semana')\n",
    "\n",
    "        \"\"\"NOTAS: se obetiene las proporciones a nivel de semana de los ultimos 60 dias de la fecha final de historia establecido en los parametros.\"\"\"\n",
    "        proportion_week_recent = historical_proportion(despachosSQL[despachosSQL['FechaEntrega'] >= (finHistoria - datetime.timedelta(criterio_historia_reciente)) - MonthBegin(1) ], ['Año', 'Mes', 'Planta'], ['Año', 'Mes', 'Planta', 'Semana_Relativa'], 'totalEntregado', ['Planta', 'Semana_Relativa'], 'semana')\n",
    "\n",
    "        \"\"\"NOTAS: se obetiene las proporciones a nivel de dia de semana de toda la historia establecida en los parametros\"\"\"\n",
    "        proportion_wday = historical_proportion(despachosSQL, ['Año', 'Mes', 'Planta'], ['Año', 'Mes', 'Planta', 'DiaSemana'], 'totalEntregado', ['Planta', 'DiaSemana'], 'dia_semana')\n",
    "\n",
    "        \"\"\"NOTAS: se obetiene las proporciones a nivel de dia de semana de los ultimos 60 dias de la fecha final de historia establecido en los parametros.\"\"\"\n",
    "        proportion_wday_recent = historical_proportion(despachosSQL[despachosSQL['FechaEntrega'] >= (finHistoria - datetime.timedelta(criterio_historia_reciente)) - MonthBegin(1) ], ['Año', 'Mes', 'Planta'], ['Año', 'Mes', 'Planta', 'DiaSemana'], 'totalEntregado', ['Planta', 'DiaSemana'], 'dia_semana')\n",
    "\n",
    "        proportion_irr = historical_proportion(despachos_irregularidades, ['Planta'], [ 'Planta', 'EtiquetaIrregularidad'], 'totalEntregado', ['Planta', 'EtiquetaIrregularidad'], 'irregularidad')\n",
    "\n",
    "        cov_plantas = stats_serie(despachosSQL[despachosSQL['FechaEntrega'] >= (finHistoria - datetime.timedelta(criterio_historia_reciente)) - MonthBegin(1) ], ['Planta'], 'totalEntregado')\n",
    "\n",
    "        cov_planta_diasemana = stats_serie(despachosSQL[despachosSQL['FechaEntrega'] >= (finHistoria - datetime.timedelta(criterio_historia_reciente)) - MonthBegin(1) ], ['Planta', 'DiaSemana'], 'totalEntregado')\n",
    "\n",
    "            #--------------- ESTIMACION DESAGREGACION FORECAST POR PLANTA ---------------# \n",
    "\n",
    "        if absorcionEstadistica == 1:\n",
    "            #obtengo el listado de plantas que han estado activas los ultimos N dias\n",
    "            DesagregacionPronosticoPlanta = despachosSQL[despachosSQL['FechaEntrega'] >= (finHistoria - datetime.timedelta(criterio_historia_reciente)) - MonthBegin(1) ]\n",
    "            DesagregacionPronosticoPlanta = pd.DataFrame({'Planta' : DesagregacionPronosticoPlanta[\"Planta\"].unique()})\n",
    "            #se divide el volumen por igual en cada planta\n",
    "            DesagregacionPronosticoPlanta['M3ForecastPlanta'] = volPais / DesagregacionPronosticoPlanta['Planta'].count()\n",
    "\n",
    "            #join con proporcion planta\n",
    "            DesagregacionPronosticoPlanta = pd.merge(DesagregacionPronosticoPlanta, proportion_plant, how='left', left_on=['Planta'], right_on=['Planta'] ).fillna(1)\n",
    "            #join con proporcion planta\n",
    "            DesagregacionPronosticoPlanta = pd.merge(DesagregacionPronosticoPlanta, cov_plantas, how='left', left_on=['Planta'], right_on=['Planta'] ).fillna(1)\n",
    "            #DesagregacionPronosticoPlanta['Aleatorio_planta'] = DesagregacionPronosticoPlanta['cov'].apply( random_number) * nivel_volatilidad\n",
    "\n",
    "            #se itera con las proporciones y se ajusta el resultado hasta que el gap sea menor a 1\n",
    "            gap_iteracion = 1000.0\n",
    "            while( abs(gap_iteracion) > 1 ):\n",
    "                #print('iteracion Planta: ' + str(gap_iteracion) )\n",
    "                DesagregacionPronosticoPlanta['forecast_planta'] = DesagregacionPronosticoPlanta['M3ForecastPlanta'] *  DesagregacionPronosticoPlanta['proportion_planta'] # *  DesagregacionPronosticoPlanta['Aleatorio_planta'] \n",
    "                resultado_iteracion = DesagregacionPronosticoPlanta['forecast_planta'].sum()\n",
    "                gap_iteracion = resultado_iteracion - volPais\n",
    "                DesagregacionPronosticoPlanta['M3ForecastPlanta'] = DesagregacionPronosticoPlanta['M3ForecastPlanta'] - (gap_iteracion / DesagregacionPronosticoPlanta['Planta'].count() )\n",
    "\n",
    "            DesagregacionPronosticoPlanta = DesagregacionPronosticoPlanta[['Planta','forecast_planta']]\n",
    "\n",
    "\n",
    "\n",
    "        #cross join tabla DesagregacionPronostico y calendario\n",
    "        calendarioLogistico['key'] = 1\n",
    "        DesagregacionPronosticoPlanta['key'] = 1\n",
    "        DesagregacionPronosticoPlantaDia = pd.merge(calendarioLogistico, DesagregacionPronosticoPlanta, on = 'key').drop(\"key\",1)\n",
    "\n",
    "        #join con proporcion semana y semana reciente\n",
    "        DesagregacionPronosticoPlantaDia = pd.merge(DesagregacionPronosticoPlantaDia, proportion_week, how='left', left_on=['Planta','Semana_relativa'], right_on=['Planta','Semana_Relativa'] ).fillna(1)\n",
    "        DesagregacionPronosticoPlantaDia = pd.merge(DesagregacionPronosticoPlantaDia, proportion_week_recent, how='left', left_on=['Planta','Semana_relativa'], right_on=['Planta','Semana_Relativa'] ).fillna(1)\n",
    "\n",
    "        #join con proporcion dia semana y dia semana reciente\n",
    "        DesagregacionPronosticoPlantaDia = pd.merge(DesagregacionPronosticoPlantaDia, proportion_wday, how='left', left_on=['Planta','Dia_Semana'], right_on=['Planta','DiaSemana'] ).fillna(1)\n",
    "        DesagregacionPronosticoPlantaDia = pd.merge(DesagregacionPronosticoPlantaDia, proportion_wday_recent, how='left', left_on=['Planta','Dia_Semana'], right_on=['Planta','DiaSemana'] ).fillna(1)\n",
    "        DesagregacionPronosticoPlantaDia.head()\n",
    "\n",
    "        #join con base de irregularidades para agregar etiqueta\n",
    "        DesagregacionPronosticoPlantaDia = pd.merge(DesagregacionPronosticoPlantaDia, irr_nacional[['FechaEntrega', 'EtiquetaIrregularidad']], how='left', left_on='Fecha de entrega', right_on='FechaEntrega' )\n",
    "\n",
    "        #join con proporcion irregularidades\n",
    "        DesagregacionPronosticoPlantaDia = pd.merge(DesagregacionPronosticoPlantaDia, proportion_irr, how='left', left_on=['Planta','EtiquetaIrregularidad'], right_on=['Planta','EtiquetaIrregularidad'] ).fillna(1)\n",
    "\n",
    "        #join con el coeficiente de variabilidad\n",
    "        DesagregacionPronosticoPlantaDia  = pd.merge(DesagregacionPronosticoPlantaDia , cov_planta_diasemana, how='left', left_on=['Planta', 'Dia_Semana'], right_on=['Planta','DiaSemana'] )\n",
    "\n",
    "        #quito columnas que no me interesan\n",
    "        DesagregacionPronosticoPlantaDia = DesagregacionPronosticoPlantaDia.drop(\"DiaSemana_x\",1)\n",
    "        DesagregacionPronosticoPlantaDia = DesagregacionPronosticoPlantaDia.drop(\"DiaSemana_y\",1)\n",
    "        DesagregacionPronosticoPlantaDia = DesagregacionPronosticoPlantaDia.drop(\"Semana_Relativa_x\",1)\n",
    "        DesagregacionPronosticoPlantaDia = DesagregacionPronosticoPlantaDia.drop(\"Semana_Relativa_y\",1) \n",
    "\n",
    "        #se agrega aleatoriedad a la desagregacion\n",
    "        DesagregacionPronosticoPlantaDia['Aleatorio'] = DesagregacionPronosticoPlantaDia['cov'].apply( random_number) * nivel_volatilidad\n",
    "\n",
    "        #SETUP MATRIZ DE ITERACIONES\n",
    "        #DesagregacionPronosticoPlantaDia['M3Forecast'] = (DesagregacionPronosticoPlantaDia['Días_Operativos'] * ( DesagregacionPronosticoPlantaDia['forecast_planta'] / DesagregacionPronosticoPlantaDia['Total_Dias_Habiles_Mes']) * DesagregacionPronosticoPlantaDia['proportion_semana'] * DesagregacionPronosticoPlantaDia['proportion_dia_semana']  ).astype(float)\n",
    "        DesagregacionPronosticoPlantaDia['M3Forecast'] =  (DesagregacionPronosticoPlantaDia['Días_Operativos'] * \\\n",
    "                                                           ( DesagregacionPronosticoPlantaDia['forecast_planta'] / DesagregacionPronosticoPlantaDia['Total_Dias_Habiles_Mes']) * \\\n",
    "                                                           DesagregacionPronosticoPlantaDia['proportion_semana_x'] * \\\n",
    "                                                           DesagregacionPronosticoPlantaDia['proportion_dia_semana_y'] * \\\n",
    "                                                           DesagregacionPronosticoPlantaDia['proportion_irregularidad'] * \\\n",
    "                                                           DesagregacionPronosticoPlantaDia['Aleatorio'] ).astype(float)\n",
    "\n",
    "        matrizPPTO_Resultado = pd.DataFrame(DesagregacionPronosticoPlantaDia.groupby('Planta')['M3Forecast'].sum()).reset_index()\n",
    "        matrizPPTO_Resultado = pd.merge(matrizPPTO_Resultado, DesagregacionPronosticoPlanta, on = 'Planta' ).drop(\"key\",1)\n",
    "        matrizPPTO_Resultado['gap'] = matrizPPTO_Resultado['M3Forecast'] - matrizPPTO_Resultado['forecast_planta']\n",
    "        matrizPPTO_Resultado['ResultadoIteracion'] = matrizPPTO_Resultado['M3Forecast']\n",
    "        matrizPPTO_Resultado['M3Forecast'] = matrizPPTO_Resultado['forecast_planta'] - matrizPPTO_Resultado['gap']\n",
    "\n",
    "        gapTotal = matrizPPTO_Resultado['gap'].abs().sum()\n",
    "\n",
    "        \n",
    "        while( gapTotal > 1 ):\n",
    "\n",
    "            #print(str(gapTotal) )\n",
    "\n",
    "            for index, row in matrizPPTO_Resultado.iterrows():\n",
    "                DesagregacionPronosticoPlantaDia.loc[DesagregacionPronosticoPlantaDia['Planta'] == row['Planta'], ['forecast_planta']] = row['M3Forecast']\n",
    "\n",
    "            #DesagregacionPronosticoPlantaDia['M3Forecast'] = ((DesagregacionPronosticoPlantaDia['forecast_planta'] / volPais ) * (DesagregacionPronosticoPlantaDia['Días_Operativos'] * ( DesagregacionPronosticoPlantaDia['forecast_planta'] / DesagregacionPronosticoPlantaDia['Total_Dias_Habiles_Mes']) * DesagregacionPronosticoPlantaDia['proportion_semana'] * DesagregacionPronosticoPlantaDia['proportion_dia_semana'])).astype(float)\n",
    "            DesagregacionPronosticoPlantaDia['M3Forecast'] = (#(DesagregacionPronosticoPlantaDia['forecast_planta'] / volPais ) *\n",
    "                                                               DesagregacionPronosticoPlantaDia['Días_Operativos'] * \n",
    "                                                               (DesagregacionPronosticoPlantaDia['forecast_planta'] / DesagregacionPronosticoPlantaDia['Total_Dias_Habiles_Mes']) * \n",
    "                                                               DesagregacionPronosticoPlantaDia['proportion_semana_x'] * \n",
    "                                                               #DesagregacionPronosticoPlantaDia['proportion_semana_y']  *  \n",
    "                                                               DesagregacionPronosticoPlantaDia['proportion_dia_semana_x'] #*\n",
    "                                                               #DesagregacionPronosticoPlantaDia['proportion_dia_semana_y'] \n",
    "                                                               #* DesagregacionPronosticoPlantaDia['proportion_irregularidad']\n",
    "                                                               #* DesagregacionPronosticoPlantaDia['Aleatorio']\n",
    "                                                               #* DesagregacionPronosticoPlantaDia['cov']\n",
    "                                                               ).astype(float) #+ (DesagregacionPronosticoPlantaDia['Días_Operativos'] * DesagregacionPronosticoPlantaDia['Aleatorio'] * (DesagregacionPronosticoPlantaDia['forecast_planta'] / DesagregacionPronosticoPlantaDia['Total_Dias_Habiles_Mes']))\n",
    "\n",
    "\n",
    "            matrizTemp = pd.DataFrame(DesagregacionPronosticoPlantaDia.groupby('Planta')['M3Forecast'].sum()).reset_index()\n",
    "\n",
    "            for index, row in matrizTemp.iterrows():\n",
    "                matrizPPTO_Resultado.loc[matrizPPTO_Resultado['Planta'] == row['Planta'], ['ResultadoIteracion']] = row['M3Forecast']\n",
    "\n",
    "            matrizPPTO_Resultado['gap'] = matrizPPTO_Resultado['ResultadoIteracion'] - matrizPPTO_Resultado['forecast_planta']\n",
    "            matrizPPTO_Resultado['M3Forecast'] = matrizPPTO_Resultado['M3Forecast'] - matrizPPTO_Resultado['gap']\n",
    "\n",
    "            gapTotal = matrizPPTO_Resultado['gap'].abs().sum()\n",
    "\n",
    "            DesagregacionPronosticoPlantaDia = DesagregacionPronosticoPlantaDia.drop(\"forecast_planta\",1)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return DesagregacionPronosticoPlantaDia\n",
    "\n",
    "\n",
    "    #ejecutar n veces la desagregacion\n",
    "\n",
    "    for i in range (0, 10):\n",
    "        for j in range(0, 1):\n",
    "            temp = generardor_desagregacion(despachosSQL, calendarioLogistico, absorcionEstadistica, ((j + 1) * 30) , inicioHistoria, finHistoria, nivel_volatilidad, volPais, pais, despachos_irregularidades, irr_nacional )\n",
    "            temp['Version'] = str(i)\n",
    "            if i == 0 :\n",
    "                desagregacion_temp = temp\n",
    "            else :\n",
    "                desagregacion_temp = pd.concat([desagregacion_temp, temp])\n",
    "\n",
    "    df_result = pd.merge (desagregacion_temp, nombre_cluster, how='left', left_on='Planta', right_on='Centro')\n",
    "    df_result = df_result[['pais','Ciudad', 'Centro', 'PlantaUnica', 'Fecha de entrega', 'M3Forecast', 'proportion_semana_x','proportion_dia_semana_y','proportion_irregularidad', 'Cluster']]\n",
    "\n",
    "    #df_result = df_result.groupby(['pais','Ciudad', 'Centro', 'PlantaUnica', 'Fecha de entrega', 'Cluster']).agg({'M3Forecast': 'mean'}).reset_index()\n",
    "    df_result = df_result.groupby(['pais','Ciudad', 'Centro', 'PlantaUnica', 'Fecha de entrega']).agg({'M3Forecast': 'mean'}).reset_index().fillna(0)\n",
    "    #df_result = df_result.groupby(['pais','Ciudad', 'Centro', 'PlantaUnica', 'Fecha de entrega','proportion_semana_x','proportion_dia_semana_y','proportion_irregularidad']).agg({'M3Forecast': 'mean'}).reset_index()\n",
    "\n",
    "\n",
    "    #Cargar desagregado a sql SOLO CUANDO SE CONFIRMA QUE EL DESAGREGADO ESTA VALIDO\n",
    "    #declarar la version, mes y año correspondiente\n",
    "   \n",
    "    database_name = 'SCAC_AV7_DesagregacionPronosticoCiudadPlantaDiaTabla' #no cambiar\n",
    "    #database_name = 'desagregadocopy'\n",
    "    #rename columns to sql columns\n",
    "    df_result = df_result.rename(columns={\n",
    "        'pais': 'Pais', \n",
    "        'Fecha de entrega': 'FechaEntrega'})\n",
    "    df_result['Version'] = version\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"sending daily forecast to sql\")\n",
    "    send_df_to_sql(df_result,database_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################### RUN FORECAST FOR MMPP ############################### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"running parameters update\")\n",
    "    #ACTUALIZAR PARAMETROS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def conectarSQL():\n",
    "        conn = mcq.ConexionSQL()\n",
    "        cursor = conn.getCursor()\n",
    "        return cursor\n",
    "\n",
    "    #Query BD SQL-Server Cemex\n",
    "    def querySQL(query, parametros):\n",
    "        #Conectar con base sql y ejecutar consulta\n",
    "        cursor = conectarSQL()\n",
    "        try:\n",
    "            cursor.execute(query, parametros)\n",
    "            #obtener nombre de columnas\n",
    "            names = [ x[0] for x in cursor.description]\n",
    "\n",
    "            #Reunir todos los resultado en rows\n",
    "            rows = cursor.fetchall()\n",
    "            resultadoSQL = []\n",
    "\n",
    "            #Hacer un array con los resultados\n",
    "            while rows:\n",
    "                resultadoSQL.append(rows)\n",
    "                if cursor.nextset():\n",
    "                    rows = cursor.fetchall()\n",
    "                else:\n",
    "                    rows = None\n",
    "\n",
    "            #Redimensionar el array para que quede en dos dimensiones\n",
    "            resultadoSQL = np.array(resultadoSQL)\n",
    "            resultadoSQL = np.reshape(resultadoSQL, (resultadoSQL.shape[1], resultadoSQL.shape[2]) )\n",
    "        finally:\n",
    "                if cursor is not None:\n",
    "                    cursor.close()\n",
    "        return pd.DataFrame(resultadoSQL, columns = names)\n",
    "\n",
    "    def generar_parametros_materiales(inicio_param, fin_param, pais_param):\n",
    "\n",
    "        lista_materiales = ['ADITIVO', 'ARENA', 'Agua', 'CEMENTO', 'CENIZA', 'GRAVA']\n",
    "\n",
    "        #Dataset de los consumos\n",
    "        df = querySQL( \"SELECT * FROM AT51_Z1045_CONSU_TICKET2 WHERE FechaInicio between ? and ?\" , (inicio_param.strftime(\"%Y-%m-%d\"), fin_param.strftime(\"%Y-%m-%d\") ) )\n",
    "        df['year_month'] = df.FechaInicio.dt.to_period('M')\n",
    "        df['TipoMaterial'] = df['TipoMaterial'].str.strip()\n",
    "        df['TextoBreveMaterial'] = df['TextoBreveMaterial'].str.strip()\n",
    "\n",
    "        #agrego informacion geografica        \n",
    "        nombre_cluster = querySQL( \"SELECT Pais, Centro, Ciudad_Cluster as Ciudad, [Desc Cluster] as Cluster, [Planta Unica] as PlantaUnica FROM SCAC_AT1_NombreCluster where Pais = ? and Activo = 1\" , (pais) )\n",
    "        cluster_planta = nombre_cluster.groupby(['Cluster', 'PlantaUnica', 'Ciudad'])['Centro'].last().reset_index()\n",
    "\n",
    "\n",
    "        #Dataset de los despachos\n",
    "        df_serv = querySQL( \"{CALL SCAC_AP10_dataset_servicios_rango (?,?,?)}\" , (pais_param, inicio_param.strftime(\"%Y-%m-%d\"), fin_param.strftime(\"%Y-%m-%d\")) )\n",
    "        df_serv = df_serv.fillna(value=np.nan)\n",
    "        df_serv['year_month'] = df_serv.FechaEntrega.dt.to_period('M')\n",
    "        #df_serv[['DescTecnica1', 'DescTecnica2', 'DescTecnica3', 'DescTecnica4', 'DescTecnica5', 'DescTecnica6', 'DescTecnica7', 'DescTecnica8', 'DescTecnica9']] = df_serv['DescTecnica'].str.split(\"-\", expand=True) \n",
    "\n",
    "        #union con el dataframe de Despacho para obtener informacion de volumen y descripcion tecnica\n",
    "        df1 = pd.merge(df, df_serv[['Cluster', 'Planta', 'Ciudad' ,'Entrega', 'VolPartida', 'TipoProducto']], on='Entrega')\n",
    "        df1 = df1[df1['VolPartida'] > 0.0]\n",
    "        df1['cantidad_por_m3'] = np.where( df1['VolPartida'] > 0.0, df1['CantidadReal']/df1['VolPartida'], 0)\n",
    "        df1['cantidad_por_m3'] = df1['cantidad_por_m3'].astype(float)\n",
    "\n",
    "        df1 = df1[df1['cantidad_por_m3'] > 0]\n",
    "\n",
    "\n",
    "        #re categorizacion de materiales\n",
    "        df1['TipoMaterial2'] = np.select(\n",
    "            [\n",
    "                (df1['TextoBreveMaterial'].str.contains('CEMENTO,BLANCO')) | (df1['TextoBreveMaterial'].str.contains('CEM ART')) | (df1['TextoBreveMaterial'].str.contains('CEM GRIS')),\n",
    "                (df1['TextoBreveMaterial'].str.contains('CENIZA')),\n",
    "                (df1['TextoBreveMaterial'].str.contains('GRAVA')),\n",
    "                (df1['TextoBreveMaterial'].str.contains('ARENA')),\n",
    "                (df1['TipoMaterial'].str.contains('ADI')),\n",
    "                (df1['TipoMaterial'].str.contains('ADC')),\n",
    "                #(df1['TipoMaterial'].str.contains('FIB')),\n",
    "                (df1['TipoMaterial'].str.contains('AGU'))\n",
    "            ],\n",
    "            [\n",
    "\n",
    "                'CEMENTO',\n",
    "                'CENIZA',\n",
    "                'GRAVA',\n",
    "                'ARENA',\n",
    "                'ADITIVO',\n",
    "                'OTROS',\n",
    "                #'FIBRA',\n",
    "                'Agua'\n",
    "            ], default = 'OTROS'#df1['TextoBreveMaterial']\n",
    "        )\n",
    "\n",
    "        df2 = pd.pivot_table(\n",
    "            df1,\n",
    "            index = ['Cluster', 'Planta', 'Ciudad','Entrega','TipoProducto'],\n",
    "            columns =['TipoMaterial2'],\n",
    "            values = ['cantidad_por_m3'],\n",
    "            aggfunc = np.sum\n",
    "        ).reset_index().set_axis(['Cluster','Planta', 'Ciudad','Entrega','TipoProducto', 'ADITIVO', 'ARENA', 'Agua', 'CEMENTO', 'CENIZA', 'GRAVA', 'OTROS'], axis=1, inplace=False)\n",
    "\n",
    "        # Definición de la distribución\n",
    "        distribucion = stats.johnsonsu\n",
    "        df_parametros = pd.DataFrame()\n",
    "\n",
    "        for i in cluster_planta.values:\n",
    "\n",
    "            for j in range(0, len(lista_materiales)):\n",
    "                datos = df2[df2['Planta'] == i[1]][[lista_materiales[j]]]\n",
    "                datos = datos.loc[datos[lista_materiales[j]].notnull()]\n",
    "\n",
    "                if (len(datos) > 0):\n",
    "                    # Ajuste para obtener el valor de los parámetros\n",
    "                    parametros = distribucion.fit(datos.to_numpy()) \n",
    "                    parametros = [*parametros]\n",
    "\n",
    "                dict_param = {'ubicacion': i[1], 'material': lista_materiales[j] ,'param1':parametros[0], 'param2':parametros[1], 'param3':parametros[2], 'param4':parametros[3] }\n",
    "\n",
    "                if len(df_parametros) == 0:\n",
    "                    df_parametros = pd.DataFrame.from_dict([dict_param])\n",
    "                else: \n",
    "                    df_parametros = df_parametros.append([dict_param], ignore_index = True)\n",
    "\n",
    "        try:\n",
    "            params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=USCLDBITVMP01; \\\n",
    "                DATABASE=BI_Tableau;UID=usertableau;PWD=usertableau$\")\n",
    "            engine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params, fast_executemany=True)\n",
    "\n",
    "            df_parametros.to_sql(\"SCAC_AT42_parametros_distribucion_materiales\", engine, \n",
    "                                 index=False, if_exists=\"replace\", schema=\"dbo\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    #PARAMETROS\n",
    "\n",
    "    fin_historia = finHistoria # fecha maxima de consulta\n",
    "    inicio_historia= fin_historia - MonthBegin(6) # fecha maxima menos n meses de historia\n",
    "\n",
    "    generar_parametros_materiales(inicio_historia, fin_historia, pais)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #MANTENIMIENTO DATATRAINING\n",
    "    print(\"running dataset maintenece\")\n",
    "\n",
    "\n",
    "\n",
    "    #Query BD SQL-Server Cemex\n",
    "    def querySQL(query, parametros):\n",
    "        #Conectar con base sql y ejecutar consulta\n",
    "        cursor = conectarSQL()\n",
    "        try:\n",
    "            cursor.execute(query, parametros)\n",
    "            #obtener nombre de columnas\n",
    "            names = [ x[0] for x in cursor.description]\n",
    "\n",
    "            #Reunir todos los resultado en rows\n",
    "            rows = cursor.fetchall()\n",
    "            resultadoSQL = []\n",
    "\n",
    "            #Hacer un array con los resultados\n",
    "            while rows:\n",
    "                resultadoSQL.append(rows)\n",
    "                if cursor.nextset():\n",
    "                    rows = cursor.fetchall()\n",
    "                else:\n",
    "                    rows = None\n",
    "\n",
    "            #Redimensionar el array para que quede en dos dimensiones\n",
    "            resultadoSQL = np.array(resultadoSQL)\n",
    "            resultadoSQL = np.reshape(resultadoSQL, (resultadoSQL.shape[1], resultadoSQL.shape[2]) )\n",
    "        finally:\n",
    "                if cursor is not None:\n",
    "                    cursor.close()\n",
    "        return pd.DataFrame(resultadoSQL, columns = names)\n",
    "\n",
    "    #SQL Methods to get operation data\n",
    "    def conectarSQL():\n",
    "        conn = mcq.ConexionSQL()\n",
    "        cursor = conn.getCursor()\n",
    "        return cursor\n",
    "\n",
    "    # dataset(pandas dataframe): base de datos con el historico\n",
    "    # array_group_top(array) : array de la jerarquia mas ALTA con el nombre de las columnas del dataset por el cual se quiere agrupar las proporciones\n",
    "    # array_group_bottom(array) : array de la jerarquia mas BAJA con el nombre de las columnas del dataset por el cual se quiere agrupar las proporciones\n",
    "    # medida_target( string ) : nombre de la columna que contiene los datos objetivo de la proporcion\n",
    "    # group_target(array) : array de nombre de columnas con las cuales queremos la proporcion final\n",
    "    # name_proportion(string) : etiqueta de la dimension a la cual le estamos calculando la proporcion\n",
    "\n",
    "    def historical_proportion( dataset, array_group_top, array_group_bottom, medida_target, group_target, name_proportion  ):\n",
    "\n",
    "        promedio_group_top = dataset.groupby(array_group_top)[medida_target].mean().reset_index()\n",
    "        promedio_group_bottom = dataset.groupby(array_group_bottom)[medida_target].mean().reset_index()    \n",
    "        proportion  = pd.merge(promedio_group_bottom, promedio_group_top, how = 'inner', left_on = array_group_top,\n",
    "                               right_on = array_group_top )\n",
    "        proportion['proportion'] = proportion[medida_target + '_x'] / proportion[medida_target + '_y']\n",
    "        proportion = proportion.groupby(group_target)['proportion'].mean().reset_index()\n",
    "        proportion.rename(columns={'proportion':'proportion_' + name_proportion}, inplace = True)\n",
    "\n",
    "        return proportion \n",
    "\n",
    "    def build_datatraining(pais, fecha_inicio, fecha_fin, fecha_split):\n",
    "        #Dataset de consu_ticket\n",
    "        df_consuticket = querySQL( \"{CALL SCAC_AP20_CONSU_TICKET_DIARIO (?,?,?)}\" , (pais, fecha_inicio, fecha_fin) )\n",
    "        df_consuticket['Centro'] = df_consuticket['Centro'].str.strip()\n",
    "        df_consuticket['TipoMaterial'] = df_consuticket['TipoMaterial'].str.strip()\n",
    "        df_consuticket['TextoBreveMaterial'] = df_consuticket['TextoBreveMaterial'].str.strip()\n",
    "\n",
    "        # Dataset calendario\n",
    "        df_calendario = querySQL( \"SELECT * FROM SCAC_AT3_DiasHabilesFuente WHERE pais = ? and [Fecha de entrega]  \\\n",
    "        between ? and ?\" , (pais, fecha_inicio, fecha_fin))\n",
    "        df_calendario.rename(columns={'Fecha de entrega':'FechaEntrega'}, inplace =True)\n",
    "\n",
    "        #Dataset de los despachos por entrega\n",
    "        df_serv = querySQL( \"{CALL SCAC_AP10_dataset_servicios_rango (?,?,?)}\" , (pais, fecha_inicio, fecha_fin) )\n",
    "        df_serv = df_serv.fillna(value=np.nan)\n",
    "        df_serv['year_month'] = df_serv.FechaEntrega.dt.to_period('M')\n",
    "\n",
    "        #Agrupacion del volumen a nivel diario y por planta unica\n",
    "        df_volumen = df_serv[(df_serv['Entrega'] != '') & (df_serv['Estatus'] == 'Normal') ].groupby(['Planta','FechaEntrega'])\\\n",
    "        ['VolPartida'].agg(['sum']).reset_index()\n",
    "        df_volumen.rename(columns={'sum':'VolumenConcreto'}, inplace =True)\n",
    "\n",
    "        df_consuticketdiario = df_consuticket[df_consuticket['CantidadReal'] > 0]\\\n",
    "        .groupby(['FechaInicio', 'Pais', 'Cluster', 'Planta', 'TipoMaterial',\n",
    "               'TextoBreveMaterial',]).agg({'CantidadReal':'sum'}).reset_index()\n",
    "\n",
    "        #Union de los dataset de mmpp, concreto y calendario\n",
    "        df_total = pd.merge(df_consuticketdiario, df_volumen, left_on=['Planta', 'FechaInicio'], right_on=['Planta','FechaEntrega'])\n",
    "        df_total = pd.merge(df_total, df_calendario, on=['FechaEntrega'])\n",
    "        df_total['year_month'] = df_total.FechaEntrega.dt.to_period('M')\n",
    "\n",
    "        ######################################################################\n",
    "        ############### CALCULO DE VARIABLES PREDICTORAS  ####################\n",
    "        ######################################################################\n",
    "\n",
    "        # Volumen(TN, L) por cantidad de material\n",
    "        df_tipoMaterial = df_total.groupby(['Planta','TipoMaterial','FechaEntrega'])['CantidadReal'].sum().reset_index()\n",
    "        df_tipoMaterial.rename(columns={'CantidadReal':'VolumenTipoMaterial'}, inplace = True)   \n",
    "\n",
    "        #proporcion de Material\n",
    "        df_proportion_material = pd.DataFrame()\n",
    "\n",
    "        for i in df_total['year_month'].unique():\n",
    "\n",
    "            mes =  df_total[df_total['year_month'] == i]['Mes'].unique()[0]\n",
    "            año =  df_total[df_total['year_month'] == i]['Año'].unique()[0]\n",
    "\n",
    "            df_param = df_total[(df_total['FechaInicio'] >= datetime.datetime(año, mes , 1) - MonthBegin(2)) & \\\n",
    "                                (df_total['FechaInicio'] < datetime.datetime(año, mes , 1))]\n",
    "            if len(df_param) > 0:\n",
    "                proportion_detalle_material = historical_proportion(df_param,['Año', 'Mes', 'Planta', 'TipoMaterial'], \\\n",
    "                                                                    ['Año', 'Mes', 'Planta', 'TipoMaterial', 'TextoBreveMaterial']\\\n",
    "                                                                    , 'CantidadReal',['Planta', 'TextoBreveMaterial'], \\\n",
    "                                                                    'material_detalle'  )\n",
    "                proportion_detalle_material['Año'] = año\n",
    "                proportion_detalle_material['Mes'] = mes\n",
    "                if len(df_proportion_material) == 0:\n",
    "                    df_proportion_material = proportion_detalle_material\n",
    "                else:\n",
    "                    df_proportion_material = pd.concat([df_proportion_material, proportion_detalle_material])\n",
    "\n",
    "        df_proportion_material = df_proportion_material.fillna(0)\n",
    "\n",
    "        #proporcion de Semana\n",
    "        df_proportion_semana = pd.DataFrame()\n",
    "\n",
    "        for i in df_total['year_month'].unique():\n",
    "\n",
    "            mes =  df_total[df_total['year_month'] == i]['Mes'].unique()[0]\n",
    "            año =  df_total[df_total['year_month'] == i]['Año'].unique()[0]\n",
    "\n",
    "            df_param = df_total[(df_total['FechaInicio'] >= datetime.datetime(año, mes , 1) - MonthBegin(2)) &  (df_total['FechaInicio'] < datetime.datetime(año, mes , 1))]\n",
    "            if len(df_param) > 0:\n",
    "                df_proportion_semana_detalle = historical_proportion(df_param,['Año', 'Mes', 'Planta', 'TextoBreveMaterial'], ['Año', 'Mes', 'Planta', 'TextoBreveMaterial', 'Semana_relativa'], 'CantidadReal',['Planta', 'TextoBreveMaterial', 'Semana_relativa'], 'semana_detalle'  )\n",
    "                df_proportion_semana_detalle['Año'] = año\n",
    "                df_proportion_semana_detalle['Mes'] = mes\n",
    "                if len(df_proportion_semana_detalle) == 0:\n",
    "                    df_proportion_semana = df_proportion_semana_detalle\n",
    "                else:\n",
    "                    df_proportion_semana = pd.concat([df_proportion_semana, df_proportion_semana_detalle])\n",
    "\n",
    "        df_proportion_semana = df_proportion_semana.fillna(0)\n",
    "\n",
    "        #proporcion de Dia Semana\n",
    "        df_proportion_diasemana = pd.DataFrame()\n",
    "\n",
    "        for i in df_total['year_month'].unique():\n",
    "\n",
    "            mes =  df_total[df_total['year_month'] == i]['Mes'].unique()[0]\n",
    "            año =  df_total[df_total['year_month'] == i]['Año'].unique()[0]\n",
    "\n",
    "            df_param = df_total[(df_total['FechaInicio'] >= datetime.datetime(año, mes , 1) - MonthBegin(2)) &  (df_total['FechaInicio'] < datetime.datetime(año, mes , 1))]\n",
    "            if len(df_param) > 0:\n",
    "                df_proportion_diasemana_detalle = historical_proportion(df_param,['Año', 'Mes', 'Planta', 'TextoBreveMaterial'], ['Año', 'Mes', 'Planta', 'TextoBreveMaterial', 'Dia_Semana'], 'CantidadReal',['Planta', 'TextoBreveMaterial', 'Dia_Semana'], 'diasemana_detalle'  )\n",
    "                df_proportion_diasemana_detalle['Año'] = año\n",
    "                df_proportion_diasemana_detalle['Mes'] = mes\n",
    "                if len(df_proportion_diasemana_detalle) == 0:\n",
    "                    df_proportion_diasemana = df_proportion_diasemana_detalle\n",
    "                else:\n",
    "                    df_proportion_diasemana = pd.concat([df_proportion_diasemana, df_proportion_diasemana_detalle])\n",
    "\n",
    "        df_proportion_diasemana = df_proportion_diasemana.fillna(0)\n",
    "\n",
    "        #Volumen mes anterior\n",
    "        df_mes_anterior = pd.DataFrame()\n",
    "\n",
    "        for i in df_total['year_month'].unique():\n",
    "\n",
    "            mes =  df_total[df_total['year_month'] == i]['Mes'].unique()[0]\n",
    "            año =  df_total[df_total['year_month'] == i]['Año'].unique()[0]\n",
    "\n",
    "            df_param = df_total[(df_total['FechaEntrega'] >= datetime.datetime(año, mes , 1) - MonthBegin(1)) &  (df_total['FechaEntrega'] < datetime.datetime(año, mes , 1))]\n",
    "            df_param = df_param.groupby(['Planta', 'TextoBreveMaterial'])['CantidadReal'].sum().reset_index()\n",
    "            df_param.rename(columns={'CantidadReal':'VolumenMesAnterior'}, inplace = True)\n",
    "            #df_param['VolumenMesAnterior'] =  df_param['VolumenMesAnterior'] \n",
    "            df_param['Año'] = año\n",
    "            df_param['Mes'] = mes\n",
    "\n",
    "            if len(df_mes_anterior) == 0:\n",
    "                df_mes_anterior = df_param\n",
    "            else:\n",
    "                df_mes_anterior = pd.concat([df_mes_anterior, df_param])\n",
    "\n",
    "        #Volumen de hace 2 meses\n",
    "        df_mes_anterior2 = pd.DataFrame()\n",
    "\n",
    "        for i in df_total['year_month'].unique():\n",
    "\n",
    "            mes =  df_total[df_total['year_month'] == i]['Mes'].unique()[0]\n",
    "            año =  df_total[df_total['year_month'] == i]['Año'].unique()[0]\n",
    "\n",
    "            df_param = df_total[(df_total['FechaEntrega'] >= datetime.datetime(año, mes , 1) - MonthBegin(2)) &  (df_total['FechaEntrega'] < datetime.datetime(año, mes , 1)  - MonthBegin(1)) ]\n",
    "            df_param = df_param.groupby(['Planta', 'TextoBreveMaterial'])['CantidadReal'].sum().reset_index()\n",
    "            df_param.rename(columns={'CantidadReal':'VolumenMesAnterior2'}, inplace = True)\n",
    "            #df_param['VolumenMesAnterior'] =  df_param['VolumenMesAnterior'] \n",
    "            df_param['Año'] = año\n",
    "            df_param['Mes'] = mes\n",
    "\n",
    "            if len(df_mes_anterior2) == 0:\n",
    "                df_mes_anterior2 = df_param\n",
    "            else:\n",
    "                df_mes_anterior2 = pd.concat([df_mes_anterior2, df_param])\n",
    "\n",
    "\n",
    "        df = pd.merge(df_total,df_tipoMaterial, on=['FechaEntrega', 'Planta','TipoMaterial'], how='left' )\n",
    "\n",
    "        df = pd.merge(df, df_proportion_material, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial'], how='left')\n",
    "\n",
    "        df = pd.merge(df, df_proportion_semana, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial','Semana_relativa'], how='left')\n",
    "\n",
    "        df = pd.merge(df, df_proportion_diasemana, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial','Dia_Semana'], how='left')\n",
    "\n",
    "        df = pd.merge(df, df_mes_anterior, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial'], how='left')\n",
    "\n",
    "        df = pd.merge(df, df_mes_anterior2, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial'], how='left')\n",
    "\n",
    "        ## queda pendiente decidir que es mejor para imputar valores\n",
    "        \"\"\"\n",
    "        df['proportion_material_detalle'] = df['proportion_material_detalle'].fillna(df['proportion_material_detalle'].median())\n",
    "        df['proportion_semana_detalle'] = df['proportion_semana_detalle'].fillna(df['proportion_semana_detalle'].median())\n",
    "        df['proportion_diasemana_detalle'] = df['proportion_diasemana_detalle'].fillna(df['proportion_diasemana_detalle'].median())\n",
    "\n",
    "        df['VolumenConcreto'] = df['VolumenConcreto'].fillna(df['VolumenConcreto'].median())\n",
    "        df['VolumenMesAnterior'] = df['VolumenMesAnterior'].fillna(df['VolumenMesAnterior'].median())\n",
    "        df['VolumenMesAnterior2'] = df['VolumenMesAnterior2'].fillna(df['VolumenMesAnterior2'].median())\n",
    "        \"\"\"\n",
    "\n",
    "        df['proportion_material_detalle'] = df['proportion_material_detalle'].fillna(0)\n",
    "        df['proportion_semana_detalle'] = df['proportion_semana_detalle'].fillna(0)\n",
    "        df['proportion_diasemana_detalle'] = df['proportion_diasemana_detalle'].fillna(0)\n",
    "\n",
    "        df['VolumenConcreto'] = df['VolumenConcreto'].fillna(0)\n",
    "        df['VolumenMesAnterior'] = df['VolumenMesAnterior'].fillna(0)\n",
    "        df['VolumenMesAnterior2'] = df['VolumenMesAnterior2'].fillna(0)\n",
    "\n",
    "        df_train = df[(df['TipoMaterial'] != 'AGU') & (df['FechaEntrega'] >= fecha_split )]\n",
    "\n",
    "        columnas_finales = ['TipoMaterial', 'TextoBreveMaterial', 'Planta', 'Cluster', 'Pais',\n",
    "               'proportion_material_detalle', 'proportion_semana_detalle',\n",
    "               'proportion_diasemana_detalle', 'CantidadReal', 'Año', 'Mes',\n",
    "               'Dia_Semana', 'Semana_relativa', 'VolumenConcreto',\n",
    "               'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2']\n",
    "\n",
    "\n",
    "        dataTraining = df_train[columnas_finales]\n",
    "\n",
    "        return dataTraining\n",
    "\n",
    "    #PARAMETROS\n",
    "    pais_param = pais\n",
    "\n",
    "    fecha_inicio_param = datetime.datetime.today() - datetime.timedelta(days=730) # esta fecha debe ser al menos 3 mese antes del fecha_split_param\n",
    "    \n",
    "    #esta fecha se incluye\n",
    "    #se define una fecha split ya que se nececita informacion de meses anteriores para calcular las variables predictoras\n",
    "    fecha_fin_param = datetime.datetime.today()\n",
    "    fecha_split_param = fecha_fin_param - datetime.timedelta(days=30)\n",
    "    fecha_split_param = fecha_split_param.strftime(\"%Y-%m-%d\")\n",
    "    fecha_fin_param = fecha_fin_param.strftime(\"%Y-%m-%d\")\n",
    "    fecha_inicio_param = fecha_inicio_param.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    \n",
    "    df_actualizar_training = build_datatraining(pais_param, fecha_inicio_param, fecha_fin_param, fecha_split_param)\n",
    "\n",
    "  \n",
    "\n",
    "    params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=USCLDBITVMP01;DATABASE=BI_Tableau;UID=usertableau;PWD=usertableau$\")\n",
    "    engine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params, fast_executemany=True)\n",
    "\n",
    "    df_actualizar_training.to_sql(\"SCAC_AT46_ciap_mmpp\", engine, index=False, if_exists=\"replace\", schema=\"dbo\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #RE-ENTRENAMIENTO MODELOS PREDICTIVOS\n",
    "\n",
    "    print(\"running RE-ENTRENAMIENTO MODELOS PREDICTIVOS\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Query BD SQL-Server Cemex\n",
    "    def querySQL(query, parametros):\n",
    "        #Conectar con base sql y ejecutar consulta\n",
    "        cursor = conectarSQL()\n",
    "        try:\n",
    "            cursor.execute(query, parametros)\n",
    "            #obtener nombre de columnas\n",
    "            names = [ x[0] for x in cursor.description]\n",
    "\n",
    "            #Reunir todos los resultado en rows\n",
    "            rows = cursor.fetchall()\n",
    "            resultadoSQL = []\n",
    "\n",
    "            #Hacer un array con los resultados\n",
    "            while rows:\n",
    "                resultadoSQL.append(rows)\n",
    "                if cursor.nextset():\n",
    "                    rows = cursor.fetchall()\n",
    "                else:\n",
    "                    rows = None\n",
    "\n",
    "            #Redimensionar el array para que quede en dos dimensiones\n",
    "            resultadoSQL = np.array(resultadoSQL)\n",
    "            resultadoSQL = np.reshape(resultadoSQL, (resultadoSQL.shape[1], resultadoSQL.shape[2]) )\n",
    "        finally:\n",
    "                if cursor is not None:\n",
    "                    cursor.close()\n",
    "        return pd.DataFrame(resultadoSQL, columns = names)\n",
    "\n",
    "    #SQL Methods to get operation data\n",
    "    def conectarSQL():\n",
    "        conn = mcq.ConexionSQL()\n",
    "        cursor = conn.getCursor()\n",
    "        return cursor\n",
    "\n",
    "\n",
    "    # Selecciono el dataset almacenado en el SQL\n",
    "\n",
    "    df = querySQL( \"SELECT * FROM SCAC_AT46_ciap_mmpp\" , () )\n",
    "\n",
    "    #ajuste de formatos\n",
    "    df['proportion_material_detalle'] = df['proportion_material_detalle'].astype(float)\n",
    "    df['proportion_semana_detalle'] = df['proportion_semana_detalle'].astype(float)\n",
    "    df['proportion_diasemana_detalle'] = df['proportion_diasemana_detalle'].astype(float)\n",
    "    df['CantidadReal'] = df['CantidadReal'].astype(float)\n",
    "    df['VolumenConcreto'] = df['VolumenConcreto'].astype(float)\n",
    "    df['VolumenTipoMaterial'] = df['VolumenTipoMaterial'].astype(float)\n",
    "    df['VolumenMesAnterior'] = df['VolumenMesAnterior'].astype(float)\n",
    "    df['VolumenMesAnterior2'] = df['VolumenMesAnterior2'].astype(float)\n",
    "    df['Año'] = df['Año'].astype(int)\n",
    "    df['Mes'] = df['Mes'].astype(int)\n",
    "    df['Dia_Semana'] = df['Dia_Semana'].astype(int)\n",
    "    df['Semana_relativa'] = df['Semana_relativa'].astype(int)\n",
    "\n",
    "    # dataframe auxiliar para codificar variables categoricas\n",
    "    df_encoding_1 = df[['TipoMaterial', 'Planta', 'Cluster', 'Pais']]\n",
    "\n",
    "    # dataframe auxiliar especifico para codificar texto breve material ordenado de menor a mayor por la suma de su volumen\n",
    "    df_encoding_2 = df.groupby(['TextoBreveMaterial'])['CantidadReal'].sum().reset_index()\n",
    "    df_encoding_2 = df_encoding_2.sort_values(by=['CantidadReal']).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    # declaracion de label encoders\n",
    "    le_tipoMaterial = preprocessing.LabelEncoder()\n",
    "    le_textoBreveMaterial = preprocessing.LabelEncoder()\n",
    "    le_planta = preprocessing.LabelEncoder()\n",
    "    le_cluster = preprocessing.LabelEncoder()\n",
    "    le_pais = preprocessing.LabelEncoder()\n",
    "\n",
    "    # fit transform de los label encoders\n",
    "    df_encoding_1['TipoMaterial'] = le_tipoMaterial.fit_transform(df_encoding_1['TipoMaterial'])\n",
    "\n",
    "    df_encoding_1['Planta'] = le_planta.fit_transform(df_encoding_1['Planta'])\n",
    "    df_encoding_1['Cluster'] = le_cluster.fit_transform(df_encoding_1['Cluster'])\n",
    "    df_encoding_1['Pais'] = le_pais.fit_transform(df_encoding_1['Pais'])\n",
    "\n",
    "    df_encoding_2['TextoBreveMaterial'] = le_textoBreveMaterial.fit_transform(df_encoding_2['TextoBreveMaterial'])\n",
    "\n",
    "    # se guardan los label encoders\n",
    "    dump(le_tipoMaterial, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_tipoMaterial.pkl', 'wb'))\n",
    "    dump(le_textoBreveMaterial, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_textoBreveMaterial.pkl', 'wb'))\n",
    "    dump(le_planta, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_planta.pkl', 'wb'))\n",
    "    dump(le_cluster, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_cluster.pkl', 'wb'))\n",
    "    dump(le_pais, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_pais.pkl', 'wb'))\n",
    "\n",
    "    # escalamiento de variables numericas\n",
    "    num_vars = ['Año', 'Mes','Dia_Semana','Semana_relativa', 'VolumenConcreto', 'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2']\n",
    "    values_scaled = df[num_vars].values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(values_scaled)\n",
    "\n",
    "    #se guarda el escalador\n",
    "    dump(min_max_scaler, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/min_max_scaler.pkl', 'wb'))\n",
    "\n",
    "    dataTraining = pd.concat([\n",
    "        df[['TipoMaterial', 'TextoBreveMaterial', 'Planta', 'Cluster', 'Pais','proportion_material_detalle', 'proportion_semana_detalle','proportion_diasemana_detalle', 'CantidadReal']].reset_index(),\n",
    "        pd.DataFrame(min_max_scaler.transform(df[num_vars].values), columns = num_vars)], \n",
    "        axis = 1, join='inner')\n",
    "\n",
    "    dataTraining = dataTraining.drop(['index'], axis = 1)\n",
    "\n",
    "    #aplicacion del label encoding\n",
    "    dataTraining['TipoMaterial'] = le_tipoMaterial.transform(dataTraining['TipoMaterial'])\n",
    "    dataTraining['TextoBreveMaterial'] = le_textoBreveMaterial.transform(dataTraining['TextoBreveMaterial'])\n",
    "    dataTraining['Pais'] = le_pais.transform(dataTraining['Pais'])\n",
    "    dataTraining['Planta'] = le_planta.transform(dataTraining['Planta'])\n",
    "    dataTraining['Cluster'] = le_cluster.transform(dataTraining['Cluster'])\n",
    "\n",
    "    # separacion variables predicotras y variable de interes\n",
    "    y = dataTraining['CantidadReal']\n",
    "    X = dataTraining.drop(['CantidadReal'], axis = 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    ## ENTRENAR RANDOM FOREST\n",
    "    modelo_rf = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "    modelo_rf.fit(X_train, y_train)\n",
    "    y_pred = modelo_rf.predict(X_test)\n",
    "\n",
    "    resultados.append({'RandomForest': np.sqrt(metrics.mean_squared_error(y_test, y_pred))} )\n",
    "\n",
    "    # guardar RandomForest\n",
    "    dump(modelo_rf, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\models/modelo_rf.sav', 'wb'))\n",
    "\n",
    "    ## ENTRENAR XGBOOST\n",
    "\n",
    "    modeloXGB = XGBRegressor()\n",
    "\n",
    "    # fit model\n",
    "    modeloXGB.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = modeloXGB.predict(X_test)\n",
    "\n",
    "    resultados.append({'XGBoost': np.sqrt(metrics.mean_squared_error(y_test, y_pred)) } )\n",
    "\n",
    "    # guardar xgboost\n",
    "    dump(modeloXGB, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\models/modelo_xgb.sav', 'wb'))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    y_train  = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    y_test = np.reshape(y_test,( int(y_test.shape[0]), 1))\n",
    "    y_train = np.reshape(y_train,( int(y_train.shape[0]), 1))\n",
    "\n",
    "    output_var = y_train.shape[1]\n",
    "    dims = X_train.shape[1]\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    # Definición red neuronal con la función Sequential()\n",
    "    model = Sequential()\n",
    "\n",
    "    # Definición de la capa densa con un tamaño de salida igual a output_var y un input_shape de dims\n",
    "    model.add(Dense(10, input_shape=(dims,),activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    #model.add(Dense(3800, input_shape=(dims,),activation='relu'))\n",
    "\n",
    "\n",
    "    model.add(Dense(output_var))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Definición de función de perdida. Se usa mean_squared_error dado que es un ejercicio de regresión\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Entrenamiento de la red neuronal con n épocas\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data = (X_test, y_test),\n",
    "              epochs=10, \n",
    "              callbacks=[PlotLossesKeras()])\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    resultados.append({'RedNeuronal': np.sqrt(metrics.mean_squared_error(y_test, y_pred)) } )\n",
    "\n",
    "    # guardar red neuronal\n",
    "    model.save(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\models/modelo_rn.h5')\n",
    "\n",
    "\n",
    "    # Selecciono el dataset almacenado en el SQL\n",
    "\n",
    "    df = querySQL( \"SELECT * FROM SCAC_AT46_ciap_mmpp\" , () )\n",
    "\n",
    "    #ajuste de formatos\n",
    "    df['proportion_material_detalle'] = df['proportion_material_detalle'].astype(float)\n",
    "    df['proportion_semana_detalle'] = df['proportion_semana_detalle'].astype(float)\n",
    "    df['proportion_diasemana_detalle'] = df['proportion_diasemana_detalle'].astype(float)\n",
    "    df['CantidadReal'] = df['CantidadReal'].astype(float)\n",
    "    df['VolumenConcreto'] = df['VolumenConcreto'].astype(float)\n",
    "    df['VolumenTipoMaterial'] = df['VolumenTipoMaterial'].astype(float)\n",
    "    df['VolumenMesAnterior'] = df['VolumenMesAnterior'].astype(float)\n",
    "    df['VolumenMesAnterior2'] = df['VolumenMesAnterior2'].astype(float)\n",
    "    df['Año'] = df['Año'].astype(int)\n",
    "    df['Mes'] = df['Mes'].astype(int)\n",
    "    df['Dia_Semana'] = df['Dia_Semana'].astype(int)\n",
    "    df['Semana_relativa'] = df['Semana_relativa'].astype(int)\n",
    "\n",
    "    # dataframe auxiliar para codificar variables categoricas\n",
    "    df_encoding_1 = df[['TipoMaterial', 'Planta', 'Cluster', 'Pais']]\n",
    "\n",
    "    # dataframe auxiliar especifico para codificar texto breve material ordenado de menor a mayor por la suma de su volumen\n",
    "    df_encoding_2 = df.groupby(['TextoBreveMaterial'])['CantidadReal'].sum().reset_index()\n",
    "    df_encoding_2 = df_encoding_2.sort_values(by=['CantidadReal']).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    # declaracion de label encoders\n",
    "    le_tipoMaterial = preprocessing.LabelEncoder()\n",
    "    le_textoBreveMaterial = preprocessing.LabelEncoder()\n",
    "    le_planta = preprocessing.LabelEncoder()\n",
    "    le_cluster = preprocessing.LabelEncoder()\n",
    "    le_pais = preprocessing.LabelEncoder()\n",
    "\n",
    "    # fit transform de los label encoders\n",
    "    df_encoding_1['TipoMaterial'] = le_tipoMaterial.fit_transform(df_encoding_1['TipoMaterial'])\n",
    "\n",
    "    df_encoding_1['Planta'] = le_planta.fit_transform(df_encoding_1['Planta'])\n",
    "    df_encoding_1['Cluster'] = le_cluster.fit_transform(df_encoding_1['Cluster'])\n",
    "    df_encoding_1['Pais'] = le_pais.fit_transform(df_encoding_1['Pais'])\n",
    "\n",
    "    df_encoding_2['TextoBreveMaterial'] = le_textoBreveMaterial.fit_transform(df_encoding_2['TextoBreveMaterial'])\n",
    "\n",
    "    # se guardan los label encoders\n",
    "    dump(le_tipoMaterial, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_tipoMaterial.pkl', 'wb'))\n",
    "    dump(le_textoBreveMaterial, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_textoBreveMaterial.pkl', 'wb'))\n",
    "    dump(le_planta, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_planta.pkl', 'wb'))\n",
    "    dump(le_cluster, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_cluster.pkl', 'wb'))\n",
    "    dump(le_pais, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_pais.pkl', 'wb'))\n",
    "\n",
    "    # escalamiento de variables numericas\n",
    "    num_vars = ['Año', 'Mes','Dia_Semana','Semana_relativa', 'VolumenConcreto', 'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2']\n",
    "    values_scaled = df[num_vars].values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(values_scaled)\n",
    "\n",
    "    #se guarda el escalador\n",
    "    dump(min_max_scaler, open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/min_max_scaler.pkl', 'wb'))\n",
    "\n",
    "    dataTraining = pd.concat([\n",
    "        df[['TipoMaterial', 'TextoBreveMaterial', 'Planta', 'Cluster', 'Pais','proportion_material_detalle', 'proportion_semana_detalle','proportion_diasemana_detalle', 'CantidadReal']].reset_index(),\n",
    "        pd.DataFrame(min_max_scaler.transform(df[num_vars].values), columns = num_vars)], \n",
    "        axis = 1, join='inner')\n",
    "\n",
    "    dataTraining = dataTraining.drop(['index'], axis = 1)\n",
    "\n",
    "    #aplicacion del label encoding\n",
    "    dataTraining['TipoMaterial'] = le_tipoMaterial.transform(dataTraining['TipoMaterial'])\n",
    "    dataTraining['TextoBreveMaterial'] = le_textoBreveMaterial.transform(dataTraining['TextoBreveMaterial'])\n",
    "    dataTraining['Pais'] = le_pais.transform(dataTraining['Pais'])\n",
    "    dataTraining['Planta'] = le_planta.transform(dataTraining['Planta'])\n",
    "    dataTraining['Cluster'] = le_cluster.transform(dataTraining['Cluster'])\n",
    "\n",
    "    # separacion variables predicotras y variable de interes\n",
    "    y = dataTraining['CantidadReal']\n",
    "    X = dataTraining.drop(['CantidadReal'], axis = 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n",
    "\n",
    "\n",
    "    n_estimators = [10,12] # number of trees in the random forest, default: 100\n",
    "    max_features = ['auto', 1] # number of features in consideration at every split, default: total\n",
    "    max_depth = [None, 12] # maximum number of levels allowed in each decision tree, default: none\n",
    "    min_samples_split = [1,2] # minimum sample number to split a node, defalut:2\n",
    "    min_samples_leaf = [1,2] # minimum sample number that can be stored in a leaf node, default:2\n",
    "    bootstrap = [True] # method used to sample data points, default:true\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "    'max_features': max_features,\n",
    "\n",
    "    'max_depth': max_depth,\n",
    "\n",
    "    'min_samples_split': min_samples_split,\n",
    "\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "    'bootstrap': bootstrap}\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "    rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n",
    "                   n_iter = 10, cv = 5, verbose=2, random_state=35, n_jobs = -1)\n",
    "\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "    # mejores parametros\n",
    "    print ('Best Parameters: ', rf_random.best_params_, ' \\n')\n",
    "\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    y_train  = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    y_test = np.reshape(y_test,( int(y_test.shape[0]), 1))\n",
    "    y_train = np.reshape(y_train,( int(y_train.shape[0]), 1))\n",
    "\n",
    "    output_var = y_train.shape[1]\n",
    "    dims = X_train.shape[1]\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    # Definición red neuronal con la función Sequential()\n",
    "    model = Sequential()\n",
    "\n",
    "    # Definición de la capa densa con un tamaño de salida igual a output_var y un input_shape de dims\n",
    "    model.add(Dense(10, input_shape=(dims,),activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    #model.add(Dense(4000, input_shape=(dims,),activation='relu'))\n",
    "\n",
    "    model.add(Dense(output_var))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Definición de función de perdida. Se usa mean_squared_error dado que es un ejercicio de regresión\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Entrenamiento de la red neuronal con n épocas\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data = (X_test, y_test),\n",
    "              epochs=5, \n",
    "              callbacks=[PlotLossesKeras()])\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #EJECUCION DEL CIAP\n",
    "\n",
    "    print(\"running EJECUCION DEL CIAP\")\n",
    "\n",
    "\n",
    "    \n",
    "    #get months\n",
    "    list_years = [start_date_forecast_str.year,end_date_forecast_str.year]\n",
    "    list_months = [start_date_forecast_str.month,end_date_forecast_str.month]\n",
    "    list_years = list(set(list_years))\n",
    "    list_months = list(set(list_months))\n",
    "    \n",
    "    #Query BD SQL-Server Cemex\n",
    "    def querySQL(query, parametros):\n",
    "        #Conectar con base sql y ejecutar consulta\n",
    "        cursor = conectarSQL()\n",
    "        try:\n",
    "            cursor.execute(query, parametros)\n",
    "            #obtener nombre de columnas\n",
    "            names = [ x[0] for x in cursor.description]\n",
    "\n",
    "            #Reunir todos los resultado en rows\n",
    "            rows = cursor.fetchall()\n",
    "            resultadoSQL = []\n",
    "\n",
    "            #Hacer un array con los resultados\n",
    "            while rows:\n",
    "                resultadoSQL.append(rows)\n",
    "                if cursor.nextset():\n",
    "                    rows = cursor.fetchall()\n",
    "                else:\n",
    "                    rows = None\n",
    "\n",
    "            #Redimensionar el array para que quede en dos dimensiones\n",
    "            resultadoSQL = np.array(resultadoSQL)\n",
    "            resultadoSQL = np.reshape(resultadoSQL, (resultadoSQL.shape[1], resultadoSQL.shape[2]) )\n",
    "        finally:\n",
    "                if cursor is not None:\n",
    "                    cursor.close()\n",
    "        return pd.DataFrame(resultadoSQL, columns = names)\n",
    "\n",
    "    #SQL Methods to get operation data\n",
    "    def conectarSQL():\n",
    "        conn = mcq.ConexionSQL()\n",
    "        cursor = conn.getCursor()\n",
    "        return cursor\n",
    "\n",
    "    # dataset(pandas dataframe): base de datos con el historico\n",
    "    # array_group_top(array) : array de la jerarquia mas ALTA con el nombre de las columnas del dataset por el cual se quiere agrupar las proporciones\n",
    "    # array_group_bottom(array) : array de la jerarquia mas BAJA con el nombre de las columnas del dataset por el cual se quiere agrupar las proporciones\n",
    "    # medida_target( string ) : nombre de la columna que contiene los datos objetivo de la proporcion\n",
    "    # group_target(array) : array de nombre de columnas con las cuales queremos la proporcion final\n",
    "    # name_proportion(string) : etiqueta de la dimension a la cual le estamos calculando la proporcion\n",
    "\n",
    "    def historical_proportion( dataset, array_group_top, array_group_bottom, medida_target, group_target, name_proportion  ):\n",
    "\n",
    "        promedio_group_top = dataset.groupby(array_group_top)[medida_target].mean().reset_index()\n",
    "        promedio_group_bottom = dataset.groupby(array_group_bottom)[medida_target].mean().reset_index()    \n",
    "        proportion  = pd.merge(promedio_group_bottom, promedio_group_top, how = 'inner', left_on = array_group_top, right_on = array_group_top )\n",
    "        proportion['proportion'] = proportion[medida_target + '_x'] / proportion[medida_target + '_y']\n",
    "        proportion = proportion.groupby(group_target)['proportion'].mean().reset_index()\n",
    "        proportion.rename(columns={'proportion':'proportion_' + name_proportion}, inplace = True)\n",
    "\n",
    "        return proportion \n",
    "\n",
    "    def gen_muestra(parametros_dist, distribucio_param, num_muestras):\n",
    "\n",
    "        muestras = distribucio_param.rvs(*parametros_dist, size=num_muestras)\n",
    "\n",
    "        muestras =  [abs(ele) for ele in muestras]\n",
    "\n",
    "        return sum(muestras)\n",
    "\n",
    "\n",
    "    def forecast_concreto(pais_param, version_param):\n",
    "        #Dataset del forecast\n",
    "        df = querySQL( \"SELECT * FROM SCAC_AV7_DesagregacionPronosticoCiudadPlantaDiaTabla WHERE Version = ? and Pais = ? \" ,(version_forecast, pais) )\n",
    "        df = df.groupby(['PlantaUnica', 'FechaEntrega'])['M3Forecast'].sum().reset_index()\n",
    "        df.rename(columns={'PlantaUnica':'Planta', 'M3Forecast': 'VolumenConcreto'}, inplace = True)\n",
    "        df = df[df['VolumenConcreto'] > 0]\n",
    "        return df\n",
    "\n",
    "    def forecast_tipo_material( df_forecast ):\n",
    "        #agrego informacion geografica        \n",
    "        nombre_cluster = querySQL( \"SELECT Pais, Centro, Ciudad_Cluster as Ciudad, [Desc Cluster] as Cluster, [Planta Unica] as PlantaUnica FROM SCAC_AT1_NombreCluster where Pais = ? and Activo = 1\" , (pais) )\n",
    "        cluster_planta = nombre_cluster.groupby(['Cluster', 'PlantaUnica', 'Ciudad'])['Centro'].last().reset_index()\n",
    "\n",
    "        #Dataset desagregacion materias primas\n",
    "        param_dist = querySQL( \"SELECT * FROM SCAC_AT42_parametros_distribucion_materiales\" , () )\n",
    "\n",
    "        # Definición de la distribución\n",
    "        distribucion = stats.johnsonsu\n",
    "        df_materiales = pd.DataFrame()\n",
    "        #materiales a desagregar\n",
    "        lista_materiales = ['ADITIVO', 'ARENA', 'Agua', 'CEMENTO', 'CENIZA', 'GRAVA']\n",
    "\n",
    "        for j in range(0, 20):\n",
    "\n",
    "            for i in cluster_planta.values:\n",
    "\n",
    "                df_forecast_detalle = df_forecast[df_forecast['Planta']==i[1]].groupby(['Planta', 'FechaEntrega'])['VolumenConcreto'].sum().reset_index()\n",
    "                df_forecast_detalle['Centro'] = i[3]\n",
    "                if(len(df_forecast_detalle) > 0 ):\n",
    "                    #ciclo para generar muestras por cada material\n",
    "                    for j in range(0, len(lista_materiales)):\n",
    "\n",
    "                        parametros_1 = param_dist[(param_dist['ubicacion']==i[1]) & (param_dist['material']==lista_materiales[j])][['param1','param2','param3','param4']].values\n",
    "\n",
    "                        if (len(parametros_1) > 0):\n",
    "                            df_forecast_detalle[lista_materiales[j]] = df_forecast_detalle.apply(lambda x: gen_muestra(tuple([float(h) for h in parametros_1[0]]), distribucion, int(x['VolumenConcreto'])), axis=1)\n",
    "\n",
    "                        else : \n",
    "                            df_forecast_detalle[lista_materiales[j]] = 0\n",
    "\n",
    "                    if len(df_materiales) == 0:\n",
    "                        df_materiales = df_forecast_detalle\n",
    "                    else: \n",
    "                        df_materiales = pd.concat([df_materiales, df_forecast_detalle])\n",
    "\n",
    "        df_materiales = df_materiales.groupby(\n",
    "            ['Planta', 'FechaEntrega', 'Centro']\n",
    "        )['ADITIVO','ARENA','Agua','CEMENTO','CENIZA','GRAVA'].median().reset_index()\n",
    "\n",
    "        df_materiales.rename(columns={ 'ADITIVO':'ADI', 'ARENA':'ARE','Agua':'AGU','CEMENTO':'CEM','CENIZA':'CEN','GRAVA':'GRA'}, inplace = True)\n",
    "\n",
    "        df_materiales = pd.melt(df_materiales, \n",
    "            id_vars=['Planta', 'FechaEntrega'], \n",
    "            value_vars = ['ADI', 'ARE','AGU', 'CEM', 'CEN', 'GRA'],\n",
    "            var_name= 'TipoMaterial',\n",
    "            value_name='VolumenTipoMaterial' )\n",
    "\n",
    "\n",
    "\n",
    "        return df_materiales\n",
    "\n",
    "\n",
    "    def dataset_principal_forecast(pais, año, mes, start_date_forecast_str, end_date_forecast_str):\n",
    "\n",
    "        fecha_inicio = fecha_referencia - datetime.timedelta(90)\n",
    "        fecha_fin = fecha_referencia \n",
    "\n",
    "        #agrego informacion geografica        \n",
    "        nombre_cluster = querySQL( \"SELECT Pais, Centro, Ciudad_Cluster as Ciudad, [Desc Cluster] as Cluster, [Planta Unica] as Planta FROM SCAC_AT1_NombreCluster where Pais = ? and Activo = 1\" , (pais) )\n",
    "        cluster_planta = nombre_cluster.groupby(['Pais','Cluster', 'Planta', 'Ciudad'])['Centro'].last().reset_index()\n",
    "  \n",
    "        # Dataset calendario\n",
    "        df_calendario = querySQL( \"SELECT * FROM SCAC_AT3_DiasHabilesFuente WHERE pais = ? and [Fecha de entrega]  between ? and ?\" , (pais, fecha_inicio.strftime(\"%Y-%m-%d\"), datetime.datetime(año, mes, 1) + MonthEnd(1) ))\n",
    "        df_calendario.rename(columns={'Fecha de entrega':'FechaEntrega'}, inplace =True)\n",
    "   \n",
    "        #Dataset de consu_ticket\n",
    "        df_consuticket = querySQL( \"{CALL SCAC_AP20_CONSU_TICKET_DIARIO (?,?,?)}\" , (pais, fecha_inicio.strftime(\"%Y-%m-%d\"), fecha_fin.strftime(\"%Y-%m-%d\")) )\n",
    "        df_consuticket['Centro'] = df_consuticket['Centro'].str.strip()\n",
    "        df_consuticket['TipoMaterial'] = df_consuticket['TipoMaterial'].str.strip()\n",
    "        df_consuticket['TextoBreveMaterial'] = df_consuticket['TextoBreveMaterial'].str.strip()\n",
    "\n",
    "     \n",
    "        \n",
    "        df_consuticketdiario = df_consuticket[df_consuticket['CantidadReal'] > 0].groupby(['FechaInicio', 'Pais', 'Cluster', 'Planta', 'TipoMaterial',\n",
    "               'TextoBreveMaterial',]).agg({'CantidadReal':'sum'}).reset_index()\n",
    "\n",
    "        #Union de los dataset de mmpp y calendario\n",
    "        df_total = pd.merge(df_consuticketdiario, df_calendario, left_on=['FechaInicio'], right_on=['FechaEntrega'], how='inner')\n",
    "        df_total['year_month'] = df_total.FechaEntrega.dt.to_period('M')\n",
    "        \n",
    "    \n",
    "\n",
    "        ######################################################################\n",
    "        ############### CALCULO DE VARIABLES PREDICTORAS  ####################\n",
    "        ######################################################################\n",
    "\n",
    "        #proporcion de Material\n",
    "        df_proportion_material = pd.DataFrame()\n",
    "\n",
    "        df_param = df_total[(df_total['FechaInicio'] >= fecha_referencia - datetime.timedelta(30) )]\n",
    "        if len(df_param) > 0:\n",
    "            proportion_detalle_material = historical_proportion(df_param,['Planta', 'TipoMaterial'], ['Planta', 'TipoMaterial', 'TextoBreveMaterial'], 'CantidadReal',['Planta', 'TextoBreveMaterial'], 'material_detalle'  )\n",
    "            proportion_detalle_material['Año'] = año\n",
    "            proportion_detalle_material['Mes'] = mes\n",
    "            if len(df_proportion_material) == 0:\n",
    "                df_proportion_material = proportion_detalle_material\n",
    "            else:\n",
    "                df_proportion_material = pd.concat([df_proportion_material, proportion_detalle_material])\n",
    "\n",
    "        df_proportion_material = df_proportion_material.fillna(0)\n",
    "     \n",
    "        #proporcion de Semana\n",
    "        df_proportion_semana = pd.DataFrame()\n",
    "\n",
    "\n",
    "        df_param = df_total[(df_total['FechaInicio'] >= fecha_referencia - datetime.timedelta(30) )]\n",
    "        if len(df_param) > 0:\n",
    "            df_proportion_semana_detalle = historical_proportion(df_param,['Planta', 'TextoBreveMaterial'], ['Planta', 'TextoBreveMaterial', 'Semana_relativa'], 'CantidadReal',['Planta', 'TextoBreveMaterial', 'Semana_relativa'], 'semana_detalle'  )\n",
    "            df_proportion_semana_detalle['Año'] = año\n",
    "            df_proportion_semana_detalle['Mes'] = mes\n",
    "            if len(df_proportion_semana_detalle) == 0:\n",
    "                df_proportion_semana = df_proportion_semana_detalle\n",
    "            else:\n",
    "                df_proportion_semana = pd.concat([df_proportion_semana, df_proportion_semana_detalle])\n",
    "\n",
    "        df_proportion_semana = df_proportion_semana.fillna(0)\n",
    "\n",
    "\n",
    "        #proporcion de Dia Semana\n",
    "        df_proportion_diasemana = pd.DataFrame()\n",
    "\n",
    "        df_param = df_total[(df_total['FechaInicio'] >= fecha_referencia - datetime.timedelta(30) )]\n",
    "        if len(df_param) > 0:\n",
    "            df_proportion_diasemana_detalle = historical_proportion(df_param,['Planta', 'TextoBreveMaterial'], ['Planta', 'TextoBreveMaterial', 'Dia_Semana'], 'CantidadReal',['Planta', 'TextoBreveMaterial', 'Dia_Semana'], 'diasemana_detalle'  )\n",
    "            df_proportion_diasemana_detalle['Año'] = año\n",
    "            df_proportion_diasemana_detalle['Mes'] = mes\n",
    "            if len(df_proportion_diasemana_detalle) == 0:\n",
    "                df_proportion_diasemana = df_proportion_diasemana_detalle\n",
    "            else:\n",
    "                df_proportion_diasemana = pd.concat([df_proportion_diasemana, df_proportion_diasemana_detalle])\n",
    "\n",
    "        df_proportion_diasemana = df_proportion_diasemana.fillna(0)\n",
    "\n",
    "        #Volumen mes anterior\n",
    "        df_mes_anterior = pd.DataFrame()\n",
    "\n",
    "        df_param = df_total[(df_total['FechaEntrega'] >= fecha_referencia - datetime.timedelta(30) ) &  (df_total['FechaEntrega'] < fecha_referencia)]\n",
    "        df_param = df_param.groupby(['Planta', 'TipoMaterial', 'TextoBreveMaterial'])['CantidadReal'].sum().reset_index()\n",
    "        df_param.rename(columns={'CantidadReal':'VolumenMesAnterior'}, inplace = True)\n",
    "        df_param['Año'] = año\n",
    "        df_param['Mes'] = mes\n",
    "\n",
    "        if len(df_mes_anterior) == 0:\n",
    "            df_mes_anterior = df_param\n",
    "        else:\n",
    "            df_mes_anterior = pd.concat([df_mes_anterior, df_param])\n",
    "\n",
    "\n",
    "        #Volumen de hace 2 meses\n",
    "        df_mes_anterior2 = pd.DataFrame()\n",
    "\n",
    "        df_param = df_total[(df_total['FechaEntrega'] >= fecha_referencia - datetime.timedelta(60) ) &  (df_total['FechaEntrega'] < fecha_referencia  - datetime.timedelta(30) )]\n",
    "        df_param = df_param.groupby(['Planta', 'TextoBreveMaterial'])['CantidadReal'].sum().reset_index()\n",
    "        df_param.rename(columns={'CantidadReal':'VolumenMesAnterior2'}, inplace = True)\n",
    "\n",
    "        df_param['Año'] = año\n",
    "        df_param['Mes'] = mes\n",
    "\n",
    "        if len(df_mes_anterior2) == 0:\n",
    "            df_mes_anterior2 = df_param\n",
    "        else:\n",
    "            df_mes_anterior2 = pd.concat([df_mes_anterior2, df_param])\n",
    "\n",
    "\n",
    "        df = df_calendario[['FechaEntrega', 'Año', 'Mes', 'Semana_relativa', 'Dia_Semana']]\n",
    "        \n",
    "        df = df[(df['Año'] == año) & (df['Mes'] == mes)].reset_index(drop = True)\n",
    "        \n",
    "        df = pd.merge(df, df_mes_anterior, on=['Año', 'Mes'])\n",
    "        df = pd.merge(df, df_mes_anterior2, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial'], how='left')\n",
    "        df = pd.merge(df, df_proportion_material, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial'], how='left')\n",
    "        df = pd.merge(df, df_proportion_semana, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial','Semana_relativa'], how='left')\n",
    "        df = pd.merge(df, df_proportion_diasemana, on=['Año', 'Mes', 'Planta', 'TextoBreveMaterial','Dia_Semana'], how='left')\n",
    "        df = pd.merge(df, cluster_planta[['Pais', 'Cluster', 'Planta']], on=['Planta'], how='left')\n",
    "\n",
    "        df['proportion_material_detalle'] = df['proportion_material_detalle'].fillna(0)\n",
    "        df['proportion_semana_detalle'] = df['proportion_semana_detalle'].fillna(0)\n",
    "        df['proportion_diasemana_detalle'] = df['proportion_diasemana_detalle'].fillna(0)\n",
    "        df['VolumenMesAnterior'] = df['VolumenMesAnterior'].fillna(0)\n",
    "        df['VolumenMesAnterior2'] = df['VolumenMesAnterior2'].fillna(0)\n",
    "\n",
    "        df_train = df[(df['TipoMaterial'] != 'AGU')]\n",
    "        return df_train\n",
    "\n",
    "\n",
    "    #run for each year and month present in forecast range\n",
    "    \n",
    "    for i in list_years:\n",
    "        for y in list_months:\n",
    "            año_target = i\n",
    "            mes_target = y\n",
    "\n",
    "            #start_date_forecast_str\n",
    "\n",
    "            fecha_referencia = datetime.datetime.today()\n",
    "            #fecha_referencia = datetime.datetime(2022, 8, 6)\n",
    "\n",
    "            version_forecast = 'AUTO_SEMANAL'\n",
    "\n",
    "\n",
    "            #1. Obtener la desagregacion del forecast de concreto\n",
    "            df_forecast_concreto = forecast_concreto(pais, version_forecast)\n",
    "\n",
    "            #2. Obtener el forecast por Tipo de Material\n",
    "            df_forecast_tipo_material = forecast_tipo_material(df_forecast_concreto)\n",
    "\n",
    "            #3. Historico de los ultimos 60 dias para completar el dataset\n",
    "            df_base = dataset_principal_forecast(pais, año_target, mes_target, start_date_forecast_str,end_date_forecast_str)\n",
    "\n",
    "            df = pd.merge(df_base, df_forecast_concreto, on=['Planta', 'FechaEntrega'], how='left')\n",
    "            df = pd.merge(df, df_forecast_tipo_material, on=['Planta', 'FechaEntrega', 'TipoMaterial'], how='left')\n",
    "\n",
    "            columnas_finales = ['TipoMaterial', 'TextoBreveMaterial', 'Planta', 'Cluster', 'Pais',\n",
    "                   'proportion_material_detalle', 'proportion_semana_detalle',\n",
    "                   'proportion_diasemana_detalle', 'Año', 'Mes',\n",
    "                   'Dia_Semana', 'Semana_relativa','VolumenConcreto', 'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2']\n",
    "\n",
    "            df = df[columnas_finales]\n",
    "\n",
    "            #Cargar label encoders y escalador\n",
    "            le_tipoMaterial = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_tipoMaterial.pkl', 'rb'))\n",
    "            le_textoBreveMaterial = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_textoBreveMaterial.pkl', 'rb'))\n",
    "            le_planta = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_planta.pkl', 'rb'))\n",
    "            le_cluster = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_cluster.pkl', 'rb'))\n",
    "            le_pais = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/le_pais.pkl', 'rb'))\n",
    "\n",
    "            min_max_scaler = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\label_encoders/min_max_scaler.pkl', 'rb'))\n",
    "\n",
    "            #aplicacion del label encoding\n",
    "            df['TipoMaterial'] = le_tipoMaterial.transform(df['TipoMaterial'])\n",
    "            df['TextoBreveMaterial'] = le_textoBreveMaterial.transform(df['TextoBreveMaterial'])\n",
    "            df['Pais'] = le_pais.transform(df['Pais'])\n",
    "            df['Planta'] = le_planta.transform(df['Planta'])\n",
    "            df['Cluster'] = le_cluster.transform(df['Cluster'])\n",
    "\n",
    "            #aplicacion del escalador\n",
    "            dataPredict = pd.concat([\n",
    "                df[['TipoMaterial', 'TextoBreveMaterial', 'Planta', 'Cluster', 'Pais','proportion_material_detalle', 'proportion_semana_detalle','proportion_diasemana_detalle']].reset_index(),\n",
    "                pd.DataFrame(min_max_scaler.transform(df[['Año', 'Mes','Dia_Semana','Semana_relativa', 'VolumenConcreto', 'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2']].values), columns = ['Año', 'Mes','Dia_Semana','Semana_relativa', 'VolumenConcreto', 'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2'])], \n",
    "                axis = 1, join='inner')\n",
    "\n",
    "\n",
    "            dataPredict = dataPredict.drop(['index'], axis = 1)\n",
    "\n",
    "            dataPredict = dataPredict.fillna(0)\n",
    "\n",
    "\n",
    "            # Cargar los modelos de regresion\n",
    "\n",
    "            m_rf = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\models/modelo_rf.sav', 'rb'))\n",
    "            m_xgb = load(open(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\models/modelo_xgb.sav', 'rb'))\n",
    "            m_rn = load_model(r'C:\\Users\\snortiz\\Documents\\projects\\weekly_forecast\\models/modelo_rn.h5')\n",
    "\n",
    "\n",
    "            ## MODELO CON RANDOM FOREST\n",
    "            y_pred_RF = m_rf.predict(dataPredict)\n",
    "            df_RF = pd.DataFrame(y_pred_RF)\n",
    "            df_RF.rename(columns={0:'PrediccionRF'}, inplace = True)\n",
    "\n",
    "            ## MODELO CON XGBOOST\n",
    "            y_pred_XG = m_xgb.predict(dataPredict)\n",
    "            df_XG = pd.DataFrame(y_pred_XG)\n",
    "            df_XG.rename(columns={0:'PrediccionXG'}, inplace = True)\n",
    "\n",
    "            ## MODELO CON REDES NEURONALES\n",
    "\n",
    "            y_pred_RN = m_rn.predict(dataPredict)\n",
    "            df_RN = pd.DataFrame(y_pred_RN)\n",
    "            df_RN.rename(columns={0:'PrediccionRNR'}, inplace = True)\n",
    "\n",
    "            result_consejo = pd.concat([ dataPredict, df_RN, df_RF, df_XG ], axis=1, join='inner')\n",
    "            result_consejo['consenso_ia'] = result_consejo[['PrediccionRNR','PrediccionRF','PrediccionXG']].median(axis=1)\n",
    "            #result_consejo['consenso_ia'] = result_consejo[['PrediccionRF','PrediccionXG']].median(axis=1)\n",
    "            #result_consejo['consenso_ia'] = result_consejo['PrediccionRF']\n",
    "\n",
    "            # inversion de las transformaciones\n",
    "            result_consejo['Cluster'] = le_cluster.inverse_transform(result_consejo['Cluster'])\n",
    "            result_consejo['Planta'] = le_planta.inverse_transform(result_consejo['Planta'])\n",
    "            result_consejo['TextoBreveMaterial'] = le_textoBreveMaterial.inverse_transform(result_consejo['TextoBreveMaterial'])\n",
    "            result_consejo['TipoMaterial'] = le_tipoMaterial.inverse_transform(result_consejo['TipoMaterial'])\n",
    "            result_consejo['Pais'] = le_pais.inverse_transform(result_consejo['Pais'])\n",
    "\n",
    "            result_consejo = pd.concat([\n",
    "                result_consejo[['TipoMaterial', 'TextoBreveMaterial', 'Planta', 'Cluster', 'Pais','proportion_material_detalle', 'proportion_semana_detalle','proportion_diasemana_detalle','PrediccionRNR','PrediccionRF','PrediccionXG', 'consenso_ia']].reset_index(),\n",
    "                pd.DataFrame(min_max_scaler.inverse_transform(result_consejo[['Año', 'Mes','Dia_Semana','Semana_relativa', 'VolumenConcreto', 'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2']].values), columns = ['Año', 'Mes','Dia_Semana','Semana_relativa', 'VolumenConcreto', 'VolumenTipoMaterial', 'VolumenMesAnterior', 'VolumenMesAnterior2'])], \n",
    "                axis = 1, join='inner')\n",
    "\n",
    "            result_consejo = result_consejo.drop(['index'], axis = 1)\n",
    "\n",
    "            # ajustes finales\n",
    "\n",
    "            df_calendario = querySQL( \"SELECT * FROM SCAC_AT3_DiasHabilesFuente WHERE pais = ? and [Fecha de entrega]  between ? and ? \" , (pais, start_date_forecast_str,end_date_forecast_str))\n",
    "            df_calendario.rename(columns={'Fecha de entrega':'FechaEntrega'}, inplace =True)\n",
    "            df_calendario.Año = df_calendario.Año.astype(str)\n",
    "            df_calendario.Mes = df_calendario.Mes.astype(str)\n",
    "            df_calendario.Dia_Semana = df_calendario.Dia_Semana.astype(str)\n",
    "            df_calendario.Semana_relativa = df_calendario.Semana_relativa.astype(str)\n",
    "\n",
    "\n",
    "            result_consejo['Version'] = version_forecast\n",
    "\n",
    "            result_consejo['Dia_Semana'] = result_consejo['Dia_Semana'].astype(float).round(0)\n",
    "            result_consejo['Dia_Semana'] = result_consejo['Dia_Semana'].astype(int)\n",
    "            result_consejo['Dia_Semana'] = result_consejo['Dia_Semana'].astype(str)\n",
    "\n",
    "            result_consejo['Semana_relativa'] = result_consejo['Semana_relativa'].astype(float).round(0)\n",
    "            result_consejo['Semana_relativa'] = result_consejo['Semana_relativa'].astype(int)\n",
    "            result_consejo['Semana_relativa'] = result_consejo['Semana_relativa'].astype(str)\n",
    "\n",
    "            result_consejo['Año'] = result_consejo['Año'].astype(float).round(0)\n",
    "            result_consejo['Año'] = result_consejo['Año'].astype(int)\n",
    "            result_consejo['Año'] = result_consejo['Año'].astype(str)\n",
    "\n",
    "            result_consejo['Mes'] = result_consejo['Mes'].astype(float).round(0)\n",
    "            result_consejo['Mes'] = result_consejo['Mes'].astype(int)\n",
    "            result_consejo['Mes'] = result_consejo['Mes'].astype(str)\n",
    "\n",
    "\n",
    "            result_consejo = pd.merge(result_consejo, df_calendario[['Año', 'Mes', 'Dia_Semana', 'Semana_relativa', 'FechaEntrega']], on=['Dia_Semana', 'Semana_relativa' ], how='left')\n",
    "            result_consejo = result_consejo[result_consejo['FechaEntrega'].dt.month == y]\n",
    "            \n",
    "            columnas_finales = ['Pais', 'Cluster', 'Planta', 'FechaEntrega','TipoMaterial', 'TextoBreveMaterial', 'PrediccionRNR', 'PrediccionRF',\n",
    "                   'PrediccionXG', 'consenso_ia', 'Version']\n",
    "\n",
    "\n",
    "            try:\n",
    "                params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=USCLDBITVMP01;DATABASE=BI_Tableau;UID=usertableau;PWD=usertableau$\")\n",
    "                engine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params, fast_executemany = True)\n",
    "\n",
    "                result_consejo[columnas_finales].to_sql(\"SCAC_AT47_desagregacion_mmpp_concreto\", \\\n",
    "                                                        engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "            \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            play_finished_alert()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    play_error_alert()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a3473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
